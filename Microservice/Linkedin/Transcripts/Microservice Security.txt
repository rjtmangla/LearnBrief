https://www.linkedin.com/learning/microservices-security/securing-microservices?contextUrn=urn%3Ali%3AlyndaLearningPath%3A645bcd56498e6459e79b3c71&u=0
Microservices: Security

Securing microservices
- [Kevin] Innovation in the microservice space has outpaced security. When teams acknowledge the security gap and attempt to secure their microservices, there's normally a lot of confusion, because it's the intersection of two deep spaces. Frustrated, they'll default to applying security tactics that worked in the past. These tactics will turn their microservices into unmanageable, distributed monoliths. Hi, I'm Kevin Bowersox. As an engineer, I've built secure microservices for systems where security was a top priority. In this course, I'll teach you patterns, standards and best practices so you can confidently integrate security into a microservice-based system without sacrificing the benefits they deliver. We'll go in depth into how platforms like API gateways and identity providers can be used to deliver secure microservices and save you time by not reinventing the wheel. So join me on my LinkedIn Learning course to learn the foundations of microservice security.

Microservices learning curve
- [Instructor] Before we take a closer look at microservice security, let's talk a bit about who the course is for and what information you should know ahead of time. When I first encountered microservice security, it was a bit overwhelming. Building microservices is already complicated. However, I was now confronted with a bunch of new security technologies, standards, and techniques. It seemed like the old playbooks were thrown out the window. At the same time, I was excited for a new challenge and I began to furiously read security specifications, talk to experts, and build prototypes. After a few years of building and securing microservice-based systems, I began to clarify the concepts for others to help them understand how to build secure microservices. So this course captures those conversations and it's intended to help anyone with a role or an interest in building a microservice-based system get past the initial information overload. Given the new way of working introduced by microservices and DevOps, this creates a pretty wide audience. I've had conversations with people in all sorts of positions, including application developers, security engineers, pipeline developers, and even scrum masters. If you're in one of these roles, you can benefit from the information in this course. Microservices and security are a pretty deep space that tends to layer concept on top of concept. I can unpack most of that information but you're going to get the most from this course if you have a little background in how systems are built and architected. If you haven't worked in a team to build a traditional information system before or haven't heard of concepts like APIs and containers, there may be other courses in the LinkedIn Learning platform that would benefit you more, like Microservice Foundations. That's all there is to know. I can guide you through the rest of the lessons from here.

Microservice Securing Foundations
Microservice concepts
- [Instructor] Microservice Architectures have introduced a fundamental shift that changes how developers build, operate, and secure technical systems. Before microservices many systems were built using a three tier architecture that consisted of a presentation layer that runs in the browser, a logic tier that ran server side and a database tier where information was stored. In this architecture the logic tier contains a set of services that work together to provide the systems business capabilities and data. In a hypothetical e-commerce system this might include services for business functions like the product catalog, inventory, pricing, shopping cart, and delivery. In a monolith we develop, package and deploy these business services as a single executable, which allows them to communicate in process to complete business functions. On the contrary microservice architectures separate the services in a systems application tier into separate components that are hosted and executed independently of one another. Since microservices execute in their own process space, all communication and collaboration between services occurs over a network protocol, typically HTTP. Access to a microservices data and capabilities occur strictly through a well-defined interface that is exposed as a rest API. Microservice architectures deliver a number of benefits that are related to their loose coupling and independence. When a system is built using microservices each service within the system can be developed and maintained by a separate team. The decoupled nature of the services allows these teams to isolate their development and avoid scheduled dependence. Once a change to a service is ready it can be least independent of the other services in the system because microservices are autonomous. The services are developed, packaged and deployed separately from each other. Eliminating the need to coordinate the timing of a big bang deployment that releases the entire system all at once. The autonomous nature of microservices also allows them to be scaled independently. If a specific service such as the catalog service comes under heavy load we can horizontally scale only that service to meet the demand. This allows us to efficiently allocate resources such as CPU and memory to where they are most needed in our system. Finally, microservice architectures can be more resilient. If a service within the distributed system begins to fail microservices can implement several fault tolerance strategies to keep the system intact. One resiliency pattern is the circuit breaker pattern which redirects traffic intended for a failing service to another service that provides a basic or default implementation of the service. These characteristics differentiate a microservice architecture from the traditional three tier monolith and will require it different security strategies be applied to achieve confidentiality integrity, and availability.

Security fundamentals
- [Instructor] Before we discuss microservice security, let's talk about several security fundamentals that apply to any system architecture. When securing an information system, there are three primary security objectives, confidentiality, integrity and availability. Confidentiality ensures that information remains private by only allowing authorized users to access it. Integrity ensures that information can be trusted because it has not been tampered with. Availability ensures that systems are accessible and can make information available to authorized users when they need it. To achieve these goals, security strategies and controls are selected and applied to a systems architecture to the extent they are required. Access control is one security technique applied to protect sensitive resources. If you've ever taken a flight, you've experienced two key components of access control, authentication and authorization. To board a flight, you must authenticate or prove your identity by showing your ID at the security gate. Next, you show your boarding pass to prove you're authorized to be on the flight. Only after completing these two steps will you be granted access to the airplane. When securing information systems, authentication is an important security control because it establishes a user's identity which is necessary to make an access decision. Systems may need to authenticate both human users and other systems, which is accomplished through the exchange of something you know such as a password or something you have such as a token. Highly sensitive systems may apply additional security measures like multi-factor authentication that combines several authentication methods such as a password and a one-time pin. Once identity is established, a system uses the authorization process to control access to resources based upon a user's privileges. Privileges are used to determine if users should be granted access to the resources they requested. Modern authorization protocols such as OAuth permit access privileges to be delegated by a resource owner to third parties, allowing the third party to access resources on behalf of a user. Trust is another very important security concept because it determines to what extent something is believed to be true. Systems must frequently decide whether to trust various pieces of information such as a user's identity, access privileges, tokens and transactional business data. This determination is made based upon factors such as the source of the data, it's age and the integrity checks it is undergone. Additionally trust domains can cause one segment of a system to trust a piece of information. However, if that data crosses a trust boundary to another segment of the system, it is no longer trusted. A system's attack surface comprises all of the paths that can be used to get data into or out of an application. A systems user interface, open ports, API, and database can all present opportunities for an attacker to compromise a system. So they are considered part of the attack surface. Reducing and hardening the attack surface is an effective strategy to enhance a system security. These security goals and concepts are universal to any architecture, but how we achieve or apply them will be drastically different from microservice architecture. Most of the strategies used to secure a monolith need to be re-engineered to secure a distributed system.


Microservice security challenges
- [Instructor] Microservice Architectures present unique security challenges that require us to rethink how we approach application security. Security strategies used in monolith are well established when compared to a Microservice Architecture. In a monolith, the number of exposed ports is kept to a minimum to reduce the applications attack surface. Requests entering the system are intercepted by a security filter that ensures users are authenticated and permitted to access the system. Upon authentication, a security context is established that contains the authenticated users identity, roles and permissions. Since the entire system executes within a single process, this security context is used throughout the monolith to authorize granular access to data and capabilities. Essentially, the entire monolith shares the same trust domain. Microservice Architectures are fundamentally different, so we need to rewrite the playbook. By design, these distributed systems are made up of independent components that must expose a port in order to be cold. This broadens the system's attack surface. The attack surface is also dynamic. It changes as we scale a service or when a new service is introduced. Access control poses another challenge as outside network traffic enters the system also known as North/South Traffic. We must decide where to perform authentication and subsequently authorization. Building authentication into each service isn't a good option, because requests from the same principle to another service would require reauthentication. Security controls may also be necessary for traffic between services also known as East/West Traffic, in order to maintain confidentiality and integrity of data pass between services. This task becomes more complicated when the traffic between services spans multiple trust domains. Once the user's identity is established, sharing that information across services while maintaining it's integrity requires specialized strategies. Microservice security isn't all doom and gloom. There are a few bright spots. Because services are independent of one another, there can be advantages when it comes to availability. For example, a denial of service attack could crash a non-essential service. However, since the attack only hit one service, it's possible for the rest of the system to remain available. If a service were exploited, the impact might be confined to a smaller portion of the system because it is harder for an attacker to move laterally. To secure a Microservice Architecture, it's important to take the time upfront to understand these unique aspects of securing a distributed system. That way the appropriate security strategies can be applied.


Identity Management and Access Control
Distributed access management patterns
- [Instructor] As microservices have matured, so have the techniques used to secure them. In security domains, such as identity and access management, you're going to even discover general patterns that have emerged, like the ones we're about to discuss. Using patterns allows us to abstract away the finer implementation details to see how a system works at a high level. Our discussion will leave a few key points for later. However, it's important we highlight two concepts before we proceed, clients and tokens. Client is a general term that describes any hardware or software calling a service. For microservice architectures, this could be a mobile device, a single page application, a Java backend, a batch process, or another company's application. Now, clients are also used by end users to access their data on APIs or microservices. We're also going to talk about tokens. You can think of these as digital keys that contain security credentials, that open doors to resources like our microservices. Like real life keys, tokens come in various formats. Some are as simple as random strings of characters, like a UUID, while others contain more detailed information. These two concepts will play a key role in our discussion of access control. Similar to other parts of a microservice architecture, the responsibility of managing a party's identity, privileges, and access is isolated to a separate distributed component. For now, we're going to call this component the identity service. The service is important because it provides tokens to client applications after an authentication event. These tokens are passed in calls to the microservices by the client to demonstrate its permission to access them. From here, the pattern can take several forms. In simple and direct implementations, microservices will pass the token back to the identity service to verify its authenticity before granting access to a resource. There's problems with this approach. It can create a lot of traffic to the identity service, and as new services are added, it becomes apparent the approach does not scale well. The identity service quickly becomes the busiest service in the system and critical issues will start to appear when the system is placed under load. One way to resolve these issues is to route traffic destined for the microservices through a reverse proxy placed in front of them. Before the reverse proxy allows any traffic through, it will validate the token against the identity service. After validation by the reverse proxy, the traffic enters what is considered a trusted portion of the network, where it can access the microservices without further scrutiny. Unfortunately, implicitly trusting the network is sort of an anti-pattern. Just establishing trust beyond the gateway introduces security concerns that are not acceptable for most systems. This leads us to the next evolution of the pattern, where the identity service places additional information regarding the identity and permissions of the end user into the token when it's created. Now, the client can pass information regarding the user and its permissions within the token to the API gateway and subsequently to the microservices. The microservices use the identity and privileges found in the token to enforce access policies without the need for any additional traffic to the identity service or just blindly trusting the network. There are a few more security details that must be applied when using any form of this pattern, but when it comes to microservices, these patterns combined with token based strategies are the most adopted when it comes to identity and access management.

Identity and Access Management platforms
- [Instructor] In a microservice architecture, the identity service provides two critical capabilities: authentication and token management. Now, as a general rule of thumb, in the security domain, you do not want to build your own solutions. You want to source them from expert providers. Luckily, there are many open-source and commercial identity and access management platforms that provide authentication and token management right out of the box. We can use these solutions in order to fulfill the capabilities required for our identity service. Now if you're in an enterprise setting, it's highly likely that your IAM platform has been selected for you. As a developer or a security engineer, you're going to find yourself integrating microservices with the IAM platform in order to secure them. From an integration perspective, we're primarily interested in four capabilities provided by these platforms: authentication, identity management, security standard implementations, and token management. Authentication is extremely complex and it's really easy to get it wrong. That's why you want to source that capability from an IAM platform. Leading platforms seamlessly integrate with many different types of identity stores in order to authenticate the user. Identity store store users credentials, and information like permissions about them that we can use to make access control decisions within a system. Identity stores range from local databases, to corporate held apps, to even social networking sites that allow users to use their credentials on that platform to authenticate in another system. For extra sensitive scenarios, IAM platforms also support multi-factor authentication. Using this technique, we combine their credentials with another form of authentication, like a one-time pin or biometrics. Identity management is another important security capability provided by these platforms. It's essential to understand who or what is accessing a resource, and specifically, what permissions they have to access them. There are many advanced scenarios for accessing a microservice that go beyond role-based permissions, such as when a user delegates permission to their data to another application. Using the identity management capabilities found in an IAM platform, we can satisfy these scenarios. These solutions are also great sources for implementations of security standards and protocols used by microservices, such as OAuth two, OpenID Connect, and JSON Web Token. These are the details I mentioned in our pattern discussion. And a one item that clearly fall into the category of concepts you need to understand, but do not want to build yourself. If you find yourself building an authorization server, it's time to take a step back and readdress the situation. Tokens are also extremely important for securing microservices, and you'll find capabilities for managing them within many identity and access management platforms. The capability to create and verify tokens is handled by an authorization server, which is defined in the OAuth two specification, and implemented in many IAM platforms. Beyond these basic capabilities, you'll find advanced support for different token formats, refresh tokens, token revocation, and token storage. Across the IAM solutions you'll find support for many different hosting configurations, including on-prem, cloud, and hybrid. These should satisfy all the different needs that you'll see across microservice-based systems. When choosing a platform, do a very careful assessment of these solutions to find the one that best suits your needs. Identity and access management platforms are the keystone of how many systems are secured. Microservices are no different. However, you'll discover they interweave many of the latest IAM capabilities in order to secure them.

API gateways
- [Instructor] The reverse proxy is another component in our pattern that provides a large set of features In a microservice-based system. A reverse proxy acts as a front end that all traffic to a server or multiple servers passes through. This helps to create a single entry point that hides details of the underlying servers from the client and allows functionality to be centralized in the gateway. In many microservice-based systems, an API gateway platform, which is one type of reverse proxy, is placed at the edge of the microservice deployment, where the traffic enters. To reach the microservices, all traffic must first pass through an API interface located in the gateway. This creates a secure access point into the microservices and shields the clients from any changes made beyond the gateway. As traffic travels through the proxy, it provides the opportunity to introduce security controls into the system. Before allowing the traffic to pass through, it is common for a gateway to verify a client's token by passing it to the IAM platform for verification. This allows the identity and access management platform to confirm the token's validity and ensure that it hasn't expired. We can also enforce access control within the gateway by protecting specific resources or endpoints. Other controls, such as spike arrest policies, focus on limiting the amount of traffic sent to the microservices by preventing large bursts of traffic from degrading API performance or causing an outage. Another policy ensures that client requests stay within a particular quota. This helps increase availability of our microservices and doesn't allow one client to consume all of the resources. Aside from policies, API gateways also provide visibility into the traffic that enters our microservices. We can monitor and trace this traffic via dashboards built into the tool, and these can be helpful when we need to investigate a security incident. Additionally, there are many features not related to security that we can interject into an API gateway, like monetization. API gateways often ship with portals, where internal and external developers can register their applications or clients to consume the APIs implemented by our microservices. This feature can integrate with the IAM platform to meet OAuth standards surrounding client registration, in order to provide a seamless and secure access to the microservices. If you've ever consumed a Facebook API or a stack overflow API, you've had to register your client, and this is the capability that a developer portal can provide. As you explore identity and access management platforms and API gateways, do not be surprised if you find a bit of overlap between their capabilities. It's not uncommon to find a little bit of OAuth in an API gateway or some reverse proxy style capabilities in an IAM platform. For smaller systems, you may determine that a single product meets your needs after a bit of research. Also, you'll find that API gateways are available in many hosting configurations, including on-prem, cloud, and hybrid models. Just make sure that whatever you choose, it adheres to your organization's policies. As you secure microservices, you'll likely find and yourself integrating one or both of these platforms into your services. They provide many security capabilities that are not practical for development teams to build on their own, but are necessary for securing microservices. We'll look deeper into these platforms to shed light on the additional security layers nested inside so that you can confidently connect the wires between your microservices and these platforms.


Access scenarios
- [Instructor] Whether you are building microservices or a client calling them through an API, it is helpful to understand the wide array of complex access scenarios that can influence your security scheme. Access to a microservice occurs through a client, which is the system making the call to the service. Microservices are not directly accessed by end users. In many cases, clients don't directly access a microservice. All of their calls are passed through the API gateway. Additional complexity is introduced when we start to consider the different types of clients that are possible. Some clients known as public clients are unable to maintain the confidentiality of their credentials. For example, single page applications are unable to store credentials securely because they run in the browser where their credentials could be accessed by an attacker. On the other hand, confidential clients can protect the confidentiality of their credentials to some degree. This is because they run in a trusted domain. These clients typically will reside on the server side where access to their internals is limited to a small group of authorized users. It's also important to consider who owns a client application. When we talk about first-party clients, we're referring to clients owned by the same organization that built a microservice. On the other hand, there can be third-party clients who are outside parties and partners that will access the microservice. This makes things harder from a security perspective because we do not control their security policies, and we cannot confirm that the security posture of their client is adequate. In general, organizations tend to make their microservice security controls more stringent for third-party clients. However, some organizations will implement a strategy where they require their own first-party clients to satisfy the same stringent security controls applied for third parties. Another thing to consider is whether client resides inside or outside of your network. Internal applications such as those used by the workforce, sit within the company firewall and can receive some benefits from their limited accessibility. External public facing applications typically face a wider audience of potential attackers, making them more accessible to attack. In any case, an application's position inside or outside of your network should never be a case for fewer security controls because of the threat of insiders or lateral attacks that can occur once the perimeter of your system is breached. When we start to think about how the end user fits into the picture, other scenarios develop. In most basic situations that involve a human end user, a first-party web application or first-party single page application will make calls to a microservice owned by the same organization that built the service. If you've adopted an IAM platform, you'll often find capabilities that integrate transparently into your architecture to handle this scenario. In more advanced situations, a human will consent to a third-party application accessing some or all of their resources on a microservice operated by a different organization. Think of it like allowing somebody to borrow your car. They don't own it, but they can use it, and maybe there's some restrictions like only to the store and back. When microservices are exposed to third parties or partners, the security behind them must be prepared to handle these types of digital delegated access scenarios. If a networking site has ever requested to access your contact lists hosted somewhere else, you've seen this scenario in action. In some scenarios, humans are not involved at all, and the communication is machine to machine similar to when using a service account. In these scenarios, microservice security must support a non-human access scenario for first-party clients, and in some cases, third-party business partners. This is actually one of the simplest scenarios to secure. Microservice deployments must support a wide array of access scenarios to keep up with the demands of an interconnected world. This makes another strong case for using pre-existing platforms and leveraging the established security standards. As we explore OAuth, we'll see how the access scenarios can determine which flow is used by a client to obtain an access token.


Work with IAM platforms
- [Instructor] In this lesson,we're going to get hands-onwith an IAM platform so that you can better understandthe underlying conceptsand how they help secure our microservices.For our exploration, we'll be using Microsoft Entra.It's one of the most widely usedidentity and access management platforms.Now Entra is really an entire familyof security services offered by Microsoft,and here you can see all of the great capabilitiesthat it delivers.The feature set that we'll be using is Microsoft Entra IDbecause it's where we find core capabilitieslike app registration and OAuth configurationsthat we'll need to secure microservices.To work with Entra ID,you'll need to log into the admin centerby navigating to entra.microsoft.comwithin your web browser.Now, if you worked with the Azure Cloud,the layout and style of this portal should feel familiar,and that's because Microsoft Entra Identityis primarily a cloud-based Identity as a Service.In the past, the service was knownas Azure Active Directory,so just know that you might encounter its former nameif you're looking at older information on the web.Over here on the sidebar,within this identity group,we'll find the capabilitiesthat we need to architect and builda secure distributed system composed of microservices.So imagine the microservices we're buildingare part of an internal systemthat's meant for use only by the employeesof an organization.An identity store is one of the core capabilitiesthat we'll need to create this systembecause we need a way to authenticate the employeesand determine if they're authorized to access the dataand capabilities that our microservices provide.With Entra, the identity store is builtright into the platform,right here in this section named users.Now this identity store containsall of the internal user accounts for the organizationand their Azure tenant.So these user identities and their associated credentialscan be used for authentication and authorization purposesacross many applications within the organization,including the microservices that we're going to secure.So let's go ahead and create a new user identitythat can access our microservice base application.To do that, we're going to click on all users,and then here within this dropdown,I'm going to select create a new user.The first thing we'll need to dois provide the user principle name of the user.This is kind of like their username.I'm going to keep it short here,and we'll just go with msecuser1for microservice security user one.Then for the display name,I'm going to specify Test User 1.And then finally, we need to provide a passwordfor this user.I'm not going to auto generate it, so I'll uncheck this box,and then here I'll provide a passwordthat I've already created.If you're following along,you'll want to take note of this passwordso that you don't forget it.Okay, everything's in place.So let's go ahead.We'll click on the review and create button.We'll take one last look at the informationto make sure everything looks good,and then we'll go ahead and create the user.Now once the user is created,we should see it appear in this list.Right there's Test User 1.Now let's go ahead,and we're going to copy the user principle namebecause we're going to attempt a login with this user.So in order to log in with the user,we can head over to the Azure portal,and once we're prompted to authenticate,I'm just going to provide the user principle name here,and then I'll click on the next buttonwhere we can provide the credentialsfor the user to sign them in.All right, then we'll be promptedfor the password we created for the user.So just go ahead and provide that and then hit sign in.Okay, so you can see that we were successful with our loginfor the user, and now it's going to have uschange their password on the first login.So let me go ahead, and I'll do that.And then after we change the password, it's going to warn usthat we need to enable multifactor authentication.Now we have 14 days to do this,so for now we're just going to disregard itby clicking on Ask later.All right, and then finally, we'll specifythat we want to stay signed in,and we have our first successful login to the Azure portalwith the new user that we created.Having identities for authenticationand authorization is one of the core building blocksyou'll need when securing a microservice.So we started with the simple stepof exploring the identity store,and we're going to expand upon thatas the course progresses.In the next chapter, we'll be coming back to Entraas part of a deeper dive into protocols like OAuthand concepts like token management,which are essential for securing a microservice.


Token Based Security and standard 
Tokens
- [Instructor] Microservices depend heavily upon tokens for establishing identity and enforcing access control. Tokens help overcome the challenges posed by a distributed system. In a microservice architecture, there is not a shared session between all of the services that can hold information about the end user and their privileges. It is also not feasible for an end user to authenticate each time a service is accessed, nor should we pass their credentials around between the services. A token overcomes these challenges, because it allows a client to hold some information that can identify the end user and the access granted to the client. It also avoids the problem of having the client hold the end user's credentials. At a high level, tokens fall under one of two formats, reference or structured. The major difference between these two tokens is where the token's actual metadata is stored. A reference token is an opaque string that doesn't contain any meaningful data. Instead, it is passed in a request and used as an identifier to look up the token metadata in a repository of stored tokens inside the IAM platform. On the other hand, structured tokens hold metadata about the token itself, the authentication event, and the end user inside the token's payload. The metadata is stored in key value pairs known as claims, which are grouped together into a claim set. One of the standards for formatting structured tokens commonly used inside microservices is JSON web token. When discussing JSON web tokens, you'll hear the term pronounced as either JWT or jot. I wouldn't get caught up in how to pronounce it. The JWT standard defines a structure that uses a signature and optional encryption to securely transmit a JSON object between two parties, like our client and our microservice. The jot structure defines three segments, a header, the payload, and a signature. The JOSE header of a jot specifies how the token was cryptographically protected, allowing the holder of the token to understand what algorithms or techniques were used to sign or encrypt the token. The payload or claim set of a jot contains information formatted in key value pairs that pertain to the end user or authentication event. The final segment of a jot, the signature, is very important. It is cryptographically derived from the token's header, payload, and a secret key. The signature is used to verify that a token hasn't changed since it was created. This is a way to ensure the token has maintained its integrity as it's passed from one party to the other. Structured tokens and reference tokens can be used for several different purposes. We have primarily been talking about access tokens, which are tokens used to gain access to a microservice, and they can be either reference or structured tokens. Another type of token, known as a refresh token, is a reference token that allows a new access token to be obtained after the current one held by a client has expired. ID tokens are another type of token that uses a jot structure to provide authentication information regarding the user for use by the client. These tokens will progress through a life cycle that begins with their issue and ends with their expiration or revocation. In the interim, they will be used by microservices to make access decisions and to establish end user identity. Most events in this lifecycle are governed by well-defined industry standards like OAuth or jot, and you can find them implemented in open sourced and commercial IAM platforms. Tokens fill the void in microservices where the session used to be in a monolith. However, the trade-off is that tokens are much more difficult to manage. In upcoming discussions, we'll layer in how various standards and cryptography are used to protect the integrity and confidentiality of tokens in microservice-based systems so that they can securely be used for access control.

OAuth 2 primer
- [Instructor] OAuth is an authorization standard that supports token based security approaches like those used for securing microservices. OAuth can be a little complicated but it's really important to understand. So we'll spend some time mastering the fundamentals before covering the details. Let's start by diving into some OAuth two terminology that defines the four roles or actors that participate in token based access strategies. First is the resource owner. Think of this as the end-user who owns the information found in a microservice and can access it. Next is the resource server which is the server hosting the API in front of the microservices. The resource server makes a resource owner's information accessible if an access token is provided. The authorization server is responsible for issuing that access token to a client and verifying the integrity of access tokens pass to the resource server. Finally, the client is an application that accesses resources on behalf of a resource owner. It is granted a token by the authorization server. These roles are part of the OAuth two standard which describes the mechanism for clients to obtain and use an access token to achieve secure access to protected resources like our microservices. How you access a hotel room is a great high level comparison to the mechanics of OAuth. As a client of the hotel, you need to access a room that is owned by the hotel owner, who would be the resource owner. The hotel staff at the front desk serves as the authorization server. They ensure the hotel owner has authorized your stay usually by confirming your payment. And if so, they issue a key card which is your token. To access the room, which is the protected resource, you must swipe the key card or token against the lock, which acts as the resource server because it protects the room from unauthorized access. If the key card is valid, your token grants you access to only your specific room for the length of your reservation. And afterwards, the card is deactivated. The hotel analogy is helpful but it can not reflect every exact detail of an OAuth flow. The OAuth two standard specifies the details of each step in one of four flows a client can follow to be issued an access token containing specific scopes. Each series of steps known as a grant type defines an ordered sequence of HTTPS calls that occur between the client, resource owner and authorization server. When requesting this token, the client specifies a parameter that indicates the scope of the access request. A scope is a protected action defined on an API that an end-user can authorize a client to perform on their behalf. You can think of OAuth as a playbook and each grant type as a play that is best suited for certain scenarios. One strength of the OAuth playbook is its flexibility, which it achieves through careful omission of certain implementation details. For example, at points in the standard, it indicates a resource owner must be identified but it clearly states the authentication mechanism is beyond the scope of the specification. It also avoids dictating the format of an access token, how a client registration occurs or how tokens should be verified. Beyond the scope of the specification is littered throughout the standard. But that's where the flexibility comes in. These careful omissions in the OAuth standard allow it to act as a fill in the blank protocol where implementers can use their preferred techniques for areas the specification leaves open, such as authentication, client registration and token format. To help fill in the blanks, the community surrounding OAuth has produced specifications like OpenID Connect, JWT, JWS and many others that extend or compliment the core OAuth standard. When securing microservices using a token, the OAuth two family of specifications and a few others play a vital role. The standards provide guidance on how to securely perform certain events throughout a token's life cycle. As we discuss access control strategies for microservices, we'll dive deeper into these standards and the role they play when integrating into our security platforms.


OAuth in Microsoft Entra
- [Instructor] In OAuth, key concepts,like the resource owner, resource server, clients,and authorization server form the backboneof how access control works.In this lesson, we'll get hands-on to explorehow some of these OAuth concepts come to life,using Microsoft Entra.So we have those four basic building blocks in OAuth,like I mentioned, the authorization server,the resource owner, the resource server, and the client.Now, within Entra,one of the major features of Microsoft Entrais that it serves as an authorization serveraccording to the OAuth protocol.That means it's capableof issuing and managing access tokens.Now, just to get a glimpse of its support,let's head over here to the application section,and then we'll go to app registrations,and then we'll navigate to the endpoints tab.Here, you'll notice in this panel,that we see some of the endpointsthat are featured in the OAuth standard,and these endpoints can be used to orchestrate flowsthat obtain an access token.Now we're going to save a deeper discussionof how tokens are issued for a more dedicated lessonbecause it's a bit involved.Now let's move on to our next concept, the resource owner,and we've already covereda little bit about how this works in Entra.Remember in the previous lessonwhere we have created this user, test user one?Well, this user is going to work with different applicationsinside of our Azure tenant,and it's going to own various resources within those apps.So for example, if our organization creates a messaging app,this user might own resources, like chat messages,that they're going to control the access to.Now those chat messages or resourcesare going to live within the resource server.Let's turn our attention to that concept.This is going to be the microservicethat's going to store the data of the resource owner,things like those chat messages.Now, to register our microserviceas a resource server within the authorization server,we can use an app registration within Microsoft Entra.So let's go back to that section,and here we can begin the processof registering our microservice with Entra.To do that, we just click on the new registration button,and then we provide a name for our application.In this case, I'm going to call it the Chat Microservice,and then here, we'll need to specifywhat type of application this is.Now, this application is going to be internal.It will only be available for users of this organization.So we're going to leave this setting,which specifies it as a single tenant app, in place.All right, from there we can go aheadand click on the register buttonto create our new app registration.Now there's some important informationthat we get on this essentials tab.You'll notice this application or client ID.This is the client IDthat is specified within the OAuth standard.So you'll want to take note of this.It's an important piece of information,and you shouldn't confuse it with the object ID.Those are two different things.Now, the next thing we're going to do is create a scopefor this microservice or resource server within Entra.You'll recall that a scopeis an action that the API exposes,so maybe something like an operationto get my messages from the microservice,or to save a new contact within the messaging microservice.The purpose of a scope is to define some protected actionthat we use for authorization decisions.To set up a scope in Entra,we can navigate to this expose and API operation blade.And within this panel, you'll see the button to add a scope.Go ahead and click on it,and you'll notice that Entra wants us firstto create this application ID URI.So this is kind of a prefix that all of our scopes createdfor this microservice will have.You can almost think of it like a namespaceor a qualifier for the microservice's scopes.We're going to go ahead and accept the default here,so we will just save and continue.And now we can proceed with creating our scope.The first thing we need to dois provide our scope with a name.I'm just going to call it chat.messages.And then we need to specify who can authorize this scopefor a client of our microservice.In this case, we're going to allow both adminsand users to authorize this scope.And then finally, we just need to providea small description for the scopeso folks can understand what they are consenting to.I'll just go ahead and specify read messages,and then I'm going to copy thatfor the display name and these descriptions.You'll want to make sure that the scope is enabledfor the app registration,and once all those things are in place,you can go ahead and click on this button to add a scope.Excellent, there we can see our new scopeand you can see that prefix, which is the app URI.So that covers three of the four major building blocksof OAuth and how they materialize in Microsoft Entra.Now we still need to discuss the final OAuth building block,which is the client.That's where we're going to pick it up in our next lesson.

Register client applications
- [Instructor] The final OAuth building blockthat we'll check out in Microsoft Entra is the client.You'll recall the client is some applicationor service that wantsto access the resource owners protected resources,which are held by the resource server,and that's our microservice.Now, clients are also registeredas app registrations in Microsoft Entra,similar to how our microservice was.However, there's a few configurations that differ.Let me show you what I mean.We're going to begin registering our clientby creating a new app registration.The first thing we'll do is give our client a name,so I'm going to call it microservice-client.We'll leave the single tenant option selectedfor the account type,and then we need to provide this configurationfor the redirect URI.This is a URI that's going to be used during OAuth flowsto exchange information between the authorization server,Entra, and the client.The first thing we need to dofor the redirect URI is select the type of platform.In this case, it's going to be web,and then here we provide the URI.Now we're going to be conducting our OAuth flowson our local workstation, so I'm just goingto specify localhost here.All right. Once that's in place,we can go ahead and click this buttonto register our new client application,and you'll notice that it also gets an applicationor client ID.This will be important during our OAuth flows.Now we're going to add some permissionsto this client application to allow it to access the scopethat we created on our microservices.To do that, we come over here to API permissions,and then we click on this button to add a permissionfor our app registration.This will open a panel that displays allof the applications used within our organization's tenant.Now, to find our microservice that we registered previously,select this API's my organization uses option,and then here, just type chat,and that will filter to our chat microservice.Now, once we select that, we can see the scopethat we created on that microservices app registration.So click on this and hit add permissions,and now you'll see that scope appearwithin the API permissions of our clients.Now, for this permission, I'm going to consentas an administrator for all of the users within the tenantby clicking this grant admin consent button.Once I do that, it's going to allow this client applicationto access the chat messagesof all the users within this organization.The final configuration we'll needto put in place is a client secret.It's a credential this client is going to useto authenticate with the authorization server.To put that in place,we'll click on the certificates & secrets blade,and then we'll click this buttonto create the new client secret.It's going to open this dialogue overon the right hand side,and we can add a description for the secret.I'll just say it's the client secret,and then we can go ahead and click on the add buttonand the secret will be generated for us.Now, you'll notice the valuefor this secret is displayed right here.At this point, you want to copy this secretbecause you will not have access to it.Again, this is a credentialand it should be handled securely.Let me go ahead and store that away.All right, so that covers the final building block of OAuthand how it works within Microsoft Entra.Let's move on to our next lesson where we'll see the rolethat these building blocks playwhen we orchestrate OAuth flows.

Issuing tokens
- [Instructor] Being issued an access token is the first step for clients that need to securely access a microservice. Tokens are issued to a client after a series of calls are orchestrated between the resource owner, the client application and the authorization server. The first part of this exchange is very important because the resource owner authenticates with the authorization server using their credentials. Then the resource owner consents to the client, accessing their protected resources. This causes the authorization server to create or mint an access token that is provided to the client. If the access token is a reference token, it must be stored at this point in time. Let's highlight a few key points and advantages of this exchange. First, the microservice is not required to store user credentials because authentication is handled centrally by the authorization server. The client never handles the credentials either because they are only provided to the authorization server. This prevents a password from being stolen from a client that stores it. It also avoids a lot of issues with information confidentiality and integrity. For example, if a client uses the resource owners credentials, the system cannot distinguish between the two of them. This complicates how system actions can be attributed to the end-user. To understand the exact details of how tokens can be issued, the OAuth two framework specifies four grant types. We'll dive into the details of the two most commonly used for microservices, the authorization code grant and the client credentials grant. Both grant types define HTTPS calls to specific end points expected on the authorization server that are used by the client in the process of obtaining a token. Before this takes place, the client and it's a redirect URI are expected to be registered with the authorization server. Registration is completed by the developer of the client application, using a registration form in the developer portal. Once the form is completed, a client ID and client secret will be issued to the application. The client's credentials in a redirect URI are used by the authorization server to authenticate the client and prevent against redirection attacks. Once these prerequisites are satisfied, clients can complete the sequence of HTTPS calls for authorization flows, like the authorization code grant. The flow starts when the resource owner visits the client website within their browser. The browser then redirects the client to the authorization end point on the authorization server. In the redirect, it passes a response type parameter to indicate it's an authorization code grant. It also passes the client ID, the redirect URI, the requested scope and a state parameter to protect against CSRF. If the resource owner is not authenticated, they will be prompted to supply their credentials within a log in page provided by the authorization server, which will be used to confirm their identity. After being authenticated, they will be asked to grant the client the requested permissions or scopes. After they have granted permission via consent page provided by the authorization server, the authorization server will redirect the resource owner's browser back to the client's redirect URI with an authorization code. From there, the client will send an HTTP post to the token endpoint on the authorization server with the authorization code it received. Other parameters like its client ID and client secret will also be sent. In the response to the post request, the authorization server will supply the client with an access token that can be used by the client to access resources on the API. Identity and access management platforms will expose the necessary authorization and token end points as part of their authorization server implementations. These allow clients to directly orchestrate authorization flows with the server. Some IAM platforms we'll build a product over top of these flows to shield first-party clients from the complexity of them. Now let's turn our attention to the client credentials grant. This grant is used to obtain an access token for machine to machine scenarios and it's much easier to orchestrate. In this flow, a registered client simply post their client ID and secret to the token endpoint, along with the specified grant type. In the response, the authorization server returns an access token that can be used to access a microservice. The OAuth's standard prescribed secure tactics for clients to obtain tokens that are issued to access protected resources. The flexibility of the standard allows IAM platforms to implement variants of the authorization server that can meet the security needs of both builders and consumers of microservices.


Work with OAuth 2.0 endpoints
- [Instructor] To get a better senseof how an OAuth flow works,we're going to secure and access a small .NET microservice.For our example, I built a small web APIthat returns a user's chat messages,which are stored in a SQL server database.So, this is the microservicethat we registered in Microsoft Entra,and now we will implement token-based authorization for it..NET Has great integration with Microsoft Entrathat makes it really simpleto achieve that for a microservice.If you've worked with any web framework before,you're probably familiar with the concept of a controller,or something similar that processes a web request.Here, in this controller,you'll notice we defined these two annotations,authorize and required scope.These allow our microserviceto authorize incoming requests using an access token.The authorized attribute will cause .NETto validate the token it receives from the client,and then, the required scope attributepoints to a scope's propertythat's defined in our app settings.json configuration file,which we can see over here.In this file, we've configured the scope propertyto require that clients accessing the web APIhave been authorized for the chat.messages scopethat we set up for the chat microservice.You'll also notice that we providethe client ID of the chat microservice,and its Azure tenant ID.We've obtained all of these valuesfrom the app registration page for the chat microservicethat's located within the Azure portal.You can see them all over here.So, now everything is in placefor this microservice to use tokensto ensure only authorized accessis permitted to the chat messages.Let's now shift our attention to the client.The client is going to access those resourceson behalf of the resource owner.To do that, it needs to obtainan access token that it can includewith the request to our chat microservice.To demonstrate this flow,I'm going to use a REST client named Insomnia.Here, in Insomnia, I've created a GET requestagainst the messages operation on our chat microservice.Now, within the auth tab,we've set up the configuration valuesthat we'll need to retrieve an access tokenvia an OAuth authorization code flow.Now, within the auth tab of the request,we've provided the necessary configuration valuesto retrieve an access tokenvia an OAuth authorization code flow.You'll recognize the fields on this tabfrom our discussion of the authorization code grant.There is the authorization URL,where the user will grant permission for the clientto access their resources.And then, you'll notice the token URL,where the client is going to exchange the code returnto the redirect URI for an access token.Now both of these standard OAuth URLsare available in the Azure portalon the app registration page that we saw before.To find them, just look through the URLs on this pageand locate the ones that apply.So, here is the OAuth 2 authorization endpoint,and you'll want to be using V2.And if we come down here,we can see the token endpoint for OAuth.To configure the authorization code flow,we also need to provide the client IDand the client's secret for the client.These values are used by the clientto authenticate itself with the authorization serverwhen requesting an access token on behalf of a user.To find the client ID, navigate tothe app registrations page for the microservice client.And then you can see here, the valuefor the application client ID.Now, we also configured a secret for this client,and I've used variables here in Insomnia to supply them,but you can directly paste those values into these fieldsin order to authenticate the client.Just be careful that you pick the right valueswithin the app registrations page,because it's easy to confuse them.You'll also notice that, within Insomnia,we provided two other values.One is the redirect URI of our microservice client,and the other is the scope that we createdon the chat microservice for reading messages.Okay, so that's where to look in Entrato find all of these configuration valuesthat you'll need for the flow.Let's go ahead and we can test this out.Imagine that our Insomnia clientis actually some web applicationthat's being used by the test userwe created earlier in Microsoft Entra.When the user hits the web app,the client's going to initiate the authorization code flowto get the user's consent to accesstheir messages stored on the microservice.So, let me go ahead and submitthe request to begin the flow.And, once we kick this off,you're going to notice a few things.First, Entra requires our test user to authenticate.It needs to establish its identity.So, let's go ahead and we can log in as our test user.Okay, now, once we are authenticated,the OAuth flow can continue,and you'll notice down herethat we have obtained this access token.And if we look at the timeline,we can see the different requests in the flowand how they occurred.And when we get to the bottom of this chain of requests,you'll see the final responsethat provides the access token here.Now, if we look at the preview,we can see that our microservice clientwas able to successfully retrieve the chat messages.And here, in the console,you can see where it passed the access tokenas a bearer token to our chat microservice.So, that's a quick look atthe OAuth authorization code flow in action.You'll notice we lean quite a biton the support provided by our IAM platformand the REST client to orchestrate this flow.



Issuing identity tokens with OIDC
- [Instructor] The OAuth two standard does leave a bit of a gap because it's solely focused on authorization. The standard does not provide any guidance on how to identify or authenticate end-users. To fill this gap, OpenID Connect was introduced. OpenID Connect is a thin identity layer that sits on top of OAuth. The standard describes how capabilities like authentication and user profile information are delivered using an authentication request, an ID token and a user info endpoint. OpenID Connect or OIDC, establishes a standalone identity provider that is separate from the client application. The architecture allows multiple client applications to leverage the same identity service for authentication. This is opposed to each one building their own and storing credentials. You've probably seen this scenario before. Organizations like LinkedIn, Google or Facebook allow applications they don't own to use their identity provider. If you've ever used a sign in with button, you've seen OpenID Connect in action. The standard is also used in enterprise scenarios, where identity providers are used to authenticate users for an organization's entire suite of applications. When we refer to an identity provider, we're referring to an authorization server that meets OIDC standards. To satisfy these standards, authorization servers must expose a new user info endpoint, in addition to the authorization and token endpoints specified in the OAuth two specification. This endpoint allows the client to pass an access token to it and in return, provides identity information about the resource owner. OpenID Connect also returns a new structured ID token, in addition to the access token from the token endpoint. It contains standard claims about the authentication event and the user's identity represented in a jot. The authorization code flow only changes slightly within OIDC. There's just a few key additions. In the initial redirect, the client adds the OpenID scope to create an authentication request. The redirect sends the resource owner to the identity provider where they're authenticated and consent to the client accessing their profile. After this, the flow continues just like it does in regular OAuth. And the authorization code is provided to the client. Finally, when the client exchanges the authorization code with the identity provider, it receives an ID token, in addition to the access token. The identity token, which is in jot format, contains a standard set of claims that provide information regarding the authentication event, the token issuer, the end-user and the token expiration. There's also three optional claims that can be included. Since the ID token is a jot, it contains a cryptographic signature that protects the token from being tampered with. Identity tokens should only be used by clients and should not be used for API access. To establish user identity within a microservice, the access token can be passed to the identity provider's user info endpoint to receive claims with information about the end-user. This allows the micro-service to get information about the end-user. To see OpenID connect in action, we can use the OpenID Connect playground provided by AuthO. This tool allows us to step through each request in the open ID connect authentication flow. The authentication flow is very similar to OAuth's authorization code flow. There's just a few key differences. Here we see the first step in the authentication flow, which is to create an authentication request. To do that, we need to make sure that we specify the OpenID scope within the request. This prompts the identity provider to authenticate the end-user. We're also able to provide more granular scopes, like profile, email, phone and address. These are going to provide the client with access to information on the end-user's profile. So once we've constructed the authentication request, we can send it to the authorization endpoint. This is going to redirect the end-user and cause them to authenticate. So here, I'm going to authenticate with Google, using an account that I created specifically for the situation. So we'll just enter in the credentials. Okay, now after we authenticate, a code is returned back to the client. The client can then exchange that code for an access token and an ID token. So let's go ahead and we're going to perform that exchange. And you'll notice that when we get the response back, we get an ID token and then, there's also an access token. Now, if we take a closer look at the ID token, you'll notice it starts with EYJ, that's a jot. Once we hit next, we're given the opportunity to verify the ID token. We can see that it was signed using the HS256 algorithm. So if we click on the verify button, we can verify the integrity of the claims within that jot. And we're also able to see them decoded. So if we take a look at those claims, you'll see information about the end-user. You can see when they were authenticated, you can see the client that sent them there and then you're going to get the standard OpenID Connect claims, here towards the bottom. So you can see how OpenID Connect augments OAuth to provide a useful identity layer on top of the protocol. The capabilities that are provided by this layer, especially the identity token and user info endpoint, provide an extremely useful specification for establishing a user context across microservices.


Token validation
- [Instructor] Tokens are sensitive pieces of information that must be validated because they grant access to the microservices that sit behind an API. In a token-based security scheme, the token is held by the client between requests and it is passed to the resource server with each request. The resource server cannot blindly trust every token it receives because an attacker may have stolen and tampered with it. In order to make an access control decision, a microservice must validate the authenticity of the token. The measures used to verify a token depend upon the type of the token. If the token is a reference token, it needs to be introspected at the authorization server. Meaning we send it back to the authorization server to learn more about the state of the token or in simple terms, we learn if it's valid. The OAuth 2.0 token Introspection Standard is an extension of OAuth that governs this process. If using introspection, it's best to centralize this feature at the API gateway. It's also important that the authorization server has clustering and caching in place so that it can scale to meet heavy loads. Validating and protecting structured tokens is a much deeper and more complicated space, which is defined by the JSON object signing and encryption specifications. These are also known as JOSE. JOSE is a group of four specifications that work together to ensure the integrity and confidentiality of a token by providing instructions for how to sign and encrypt them. JSON web token defines the token structure and it's at the core of the JOSE specifications. The JSON web signature specification outlines the steps required to create the JWT signature which further references the JSON web algorithms and JSON web key standards. If you need to encrypt the JWT to protect sensitive claims, you can use the JSON web encryption standard. The easiest way to understand the JOSE specifications is to see them in action. So we're going to use a tool provided by the jwt.io website in order to take a closer look. If we navigate down to the middle of the page, we'll see the tool. It allows us to encode and decode a JWT. Now, if you take a look at the left side of the screen, you'll see your first JWT. And it's easy to tell it's a JWT because of the first three characters EYJ. JWTs are Base64 URL encoded. So it's going to look like a random string of characters, but EYJ is the Base64 encoding for an opening curly brace and a quotation mark which are the characters that all JWTs start with. So if you see that, you're most likely working with a JWT. Now, if we look at this dropdown at the top of the page, you're going to see a list of algorithms. These come from the JSON Web algorithm specification and these are the options that we have for signing the JWT. In general, each of the algorithms uses the same computation to create the signature with just some different inputs. It takes the header and Base64 URL encodes it, concatenates it with a period. And then Base64 URL encodes the payload, and concatenates that. It then takes a key which it uses for the algorithm that computes the signature and that gets appended to the end of the JWT. Let's take a look at this in action. So we're going to use the HS256 algorithm that is specified at the top of the page by default. This is a symmetric key algorithm. So it's going to use the same key in order to create the signature as it's going to use to verify it. So let's just enter a really simple key here. I'm going to name it Mykey and then I'm going to change the JWT just a little bit. You'll notice it's changing on the left-hand side as it creates the new JWT. So let's put my name there. You can put yours if you would like. But in the end, we wind up with this new token. Go ahead and copy that token. Don't forget your key and then hit the refresh key. So a JWT allows us to verify that the token has not been tampered with, as it goes from one party to another. So let's imagine that we're the party receiving this JWT. And let's say that we put the wrong key in here. And then we paste the JWT into the tool. You'll notice at the bottom, it's going to alert us to the fact that the signature is invalid. So let's run this scenario again and I'm going to refresh the page and then hopefully I'll remember my key. So let's put it into the bottom of the page where the signature is verified. And now I'm going to go ahead and paste the JWT one more time. You'll notice this time, the signature verifies. Now, depending upon which algorithm you use, you might need a different key. So if we take a look at an RS256 algorithm you'll notice it's going to look for a public and private key. So you'll need those to create the signature. Now, if you receive this JWT, you're going to need the public key in order to verify the signature. And one way that's provided by an identity provider is through a JSON web key set that's typically exposed via an endpoint. We can take a look at an example. This is the JSON web key set from Auth0 from one of the demonstrations they provide. Now, it looks a little bit cryptic. So what I'm going to do, is take the JSON and go over to a lina. And then here, I'm going to validate the JSON. And you'll notice that we get this array of keys. So this is a JSON web key set and inside that array, is a JSON web key. These are all specified in the specifications that accompany JOSE. If you receive a JWT and you have the JSON web key set, you can go ahead and select the corresponding key and use that to verify the signature to ensure the JWT hasn't been tampered with.


Token maintenance and protection
- [Instructor] After a token has been issued, a few maintenance activities may take place before the token reaches its end of life. To help manage these activities, the identity and access management platform may add some token management features that are optional or not completely defined for authorization servers in the OAuth specification. In the best case scenario, a token is used for appropriate access by a client until it reaches its expiration date. At this point, it no longer can be used for access to a microservice. This is typically achieved by setting an expires in claim on the access token or storing information regarding its expiration date. In general, it is best to keep the access token short-lived, so that if it were to be compromised, it doesn't provide access for a long period of time. In some scenarios, clients may be provided with a refresh token that allows them to obtain a new access token without the resource owner involved. This is helpful when the client needs to access information when the resource owner is offline. Now there's one big problem with refresh tokens. If they're stolen, they're very similar to a password. To help mitigate the threat of a stolen refresh token, they can be rotated by issuing a new refresh token every time an access token is obtained using a refresh token. They just continue to rotate. In the worst case scenario, access tokens or refresh tokens are compromised, because they're not well-protected, and they need to be revoked. This is not an easy task, because it requires tokens to be persisted into storage and may require all tokens consented to by a user to be revoked. If tokens are not stored, it may not be possible to revoke them until all of the tokens have expired. In other situations, revocation can occur naturally, like when a logout occurs. Sometimes a client is unregistered, or a resource owner revokes a client's access to their resources. To support these scenarios, the token revocation specification extends OAuth by introducing an endpoint that allows a token to be revoked. After the token's revoked, it can no longer be used. Finally, let's just cover a few general best practices for working with tokens. It's almost a universal standard at this point, but I do want to point out that all of the calls that we talked about in OAuth should take place over TLS to prevent credentials or tokens from being compromised. You should never transport any of this information in the clear. Additionally, tokens should be protected like passwords, because anybody with access to a token can access a microservice. The same sort of protection should apply to the client ID and client secret issued to a client. Also, be sure to include an expiration date on every access token, keep the expiration short, and use refresh tokens if necessary. Without an expiration, you could be issuing access to a resource forever, which is not ideal. Finally, don't include highly sensitive information inside the payload of a token that leaves your control. If you need to access sensitive information from a microservice, there's patterns that can be used to secure the information. We've discussed the general purpose of tokens, their life cycle, and a few best practices. Understanding how they are used, when they are used, and the different standards that apply will allow you to be certain your microservices are securely using them.


Securing Between Microservice
MTLS
- [Instructor] Identity plays an important role in digital security and microservices are not an exception. Some microservice security schemes choose to simply trust the network after a request has passed a certain point in the architecture. Services within a certain segment of the network are simply permitted to call each other without any scrutiny of the request or identification of the calling party. This breaks the important concept of defense in depth and creates a major vulnerability if an attacker breaches the network because they now have unimpeded access to the microservices in a deployment. Large segments of the network shouldn't be established as trust boundaries. Instead, the trust boundary should be the microservice itself. The microservice should not trust any external information or parties by default. Instead, the microservice should verify the identity of any calling party and the integrity of the information it receives. This is the concept of zero trust and we can incorporate it into how our microservices communicate to add some depth to our defense. Digital certificates are the most common way to assert the identity of a digital entity. A digital certificate contains information that identifies an entity, a public key and information regarding the certificate authority that issued it. in transport layer security or TLS, a digital certificate and PKI allow two parties to communicate information via an encrypted channel. This is established using a certificate and an encryption key that are exchanged. In the scenario, it's only one way authentication using the certificate because the server hasn't verified the identity of the client using its digital cert. It's only the client that verifies the server's identity, checking that its certificate was issued by a trusted list of certificate authorities maintained in the browser. You've most likely seen this in your browser when visiting a sensitive website, like an online banking platform. A little lock displaced to the left of the URL, where you can see the website cert. The digital certificate allows you to verify that it's actually your bank's website and not a spoof site that tends to steal your credentials. Because our browser trusts the certificate authority that issued the bank cert, we know the site can be trusted. Additionally, it's an indicator that our browser is communicating with the web server securely over TLS. So we can be sure that any information we send is not in clear text. To secure microservices, we can take the concept of TLS one step further by implementing mutual TLS. When using mutual TLS, both parties authenticate by providing their digital certificate that was issued by a mutually trusted certificate authority before the secure channel is established. Mutual TLS is one of the most common approaches for securing microservices use today. It allows the services to identify each other and be confident in each other's identity before communicating. When using mutual TLS, each microservice is deployed with a certificate and private and public key pair that allows them to identify each other when they communicate and to establish secure connections. Additionally, mutual TLS should be used to secure the communication between the API gateway and the microservice deployment. This adds an extra level of protection because if an attacker found their way into the network, they couldn't call a microservice without a trusted digital certificate. This is the depth in our defense. One of the challenges with mutual TLS is management and provisioning of the certificates. Most microservices are hosted in a femoral containers that will spin up and down on a whim. So provisioning a certificates needs to be automated. There is also the problem of how we rotate certificates that are about to expire. This should also be handled via automation because of the number of instances we can spin up of a microservice. While this may seem overwhelming, there are container orchestrators and service meshes that have built in features for managing mutual TLS. Configuring mutual TLS capabilities adds another layer of security to your microservices that is extremely effective and worth the effort. If you choose to trust the network, be aware you're putting your microservices at risk.


Securing east to west traffic
- [Instructor] Well-designed microservices are created with the single responsibility pattern in mind. They do one thing and they do it well. And hearing to this principle creates the need for microservices to consume other microservices. This generates East, West traffic inside of the cluster of services. The sideways traffic between the microservices creates new security considerations, such as how we identify a user and how we make access decisions within the cluster of services. Let's consider a simple e-commerce scenario where a customer is placing an order. We'll assume the customer has already authenticated and the client owned by the e-commerce company is holding a valid access token. To place the order, the order service will need to call the payment service, which can issue a financial transaction using the account details of the customer who is our resource owner. One anti-pattern is to use a single access token with a shared scope that allows access to both services. This isn't ideal because it starts to form a distributed monolith where the scopes use to protect both services must align and we start to build tight coupling. We also violate the concept of lease privilege because the access token could be used to call the more sensitive payment service directly by the client or even the resource owner who has access to the access token. One strategy to avoid this situation is for the order service to perform a client credentials grant to obtain a new access token prior to calling the payment service. This allows the order service and the payment service to have decoupled scopes. The new access token with the payment scope is only held by the order service which avoids providing the client with elevated access. Unfortunately, while this approach is somewhat better it still takes us closer to a distributed monolith. Without the original tokens, the payment service has no awareness of the end user's identity. It is forced to defer access control decisions to the order service because the order service can obtain the identity of the end user using something like the sub-claim on a JWT or using the OIDC user info endpoint. Without any information about the end user's identity, the payment service has no choice but to trust the calls it receives from the order service. If an attacker or an insider could compromise the order service, they could issue a payment from any of the customers. To solve this problem, we can use a JWT containing claims about the end-user that is passed along with our new access token from the order service to the payment service. Sometimes this is done by nesting one token inside of the other. The payment service can read these claims after verifying the integrity of the token using at signature. With this information, the payment service can identify the user and make a decision about whether or not to authorize the payment that's been requested. When using this approach, we can apply a pattern that helps us protect sensitive information about our end-user. In this pattern, the client is issued a reference token that it holds to access the microservices. When the reference token enters the network through the API gateway, it is passed to the IAM platform where it is switched from a reference token into a structured token. This structured token or JWT contains claims about the identity of the end-user. This approach better protects information about the end-user because it never sends a structured token with claims to the client. However, it does exchange the reference token for a structured token at the API gateway that can be passed back to our microservices, which use it to identify the end-user. These techniques allow microservices to securely communicate and helps establish a user context without any state stored between the services. When building microservices that must collaborate, it's important to closely examine calls from one service to another to ensure scope or claims based security decisions do not become intertwined to couple the services together.


Monitoring and logging
- [Instructor] Being able to observe the activity taking place in a system is important for many reasons. However, from a security perspective, it's essential. Having visibility into a system through its logs can alert us to suspicious behavior that detects an ongoing attack. If a security incident is discovered in a system that has good logging, it's possible to identify the attack vector, the extent of the attack, and possibly the attacker. Logs can also serve as great mechanisms for establishing audit trails for sensitive operations that may be exploited by insiders. Like most other security concerns, microservices present a unique challenge, because sensitive operations that once occurred in process now span multiple distributed components. This causes logging to be fragmented across the multiple services, and discrepancies can arise between logging strategies used by different teams. Tracing is one of the most important techniques that we can use to overcome these challenges related to logging in microservice architectures. Using a correlation ID associated with each request, we can trace an event that is distributed over several microservices. When each microservice includes the correlation ID of the event in their logs, the event can be reassembled at a later point in time using a common thread of information. This is helpful for aggregating system activities into a single event. We can then use this information to support a security inquiry or to determine what went wrong in an incident. To make logs more useful and easy to consume, a common logging structure should be established across the teams that build your microservices. Typically, it's standard to include information about when an audited event took place, who was involved, what type of event it was, and what section of the code invoked it. If you're using the same language across your microservices, I would highly recommend deciding upon a standard logging framework and mandating a configuration file that establishes the logging pattern used across your teams that build microservices. Teams should also remember to log unsuccessful activities that were attempted as well, because these events like authorization failures, error in HTTP codes, and invalid endpoint URLs, can indicate an attempted attack or an attacker running a scan. Since microservices are distributed, and there may be many instances of a single microservice, all of your logs should be sent to a central host where they're collected and aggregated. This allows all of the log information to be gathered in one spot where it can be searched to provide a complete picture of activity into the distributed system. There are many open-source and commercial tools available to perform this job. If you're interested in an open-source tool, check out the elastic stack platform. it's very feature-rich and includes capabilities for monitoring, which we're going to discuss next. Once the information from your microservices is logged centrally, automated monitoring can identify and alert developers to suspicious behavior in the system. These alerts typically focus on traffic that fails due to mutual TLS, invalid access tokens, or excessive traffic which can be an indicator of an attacker probing your system. Once detected, alerts can be issued through an instant response system that proceeds through a series of escalated notifications to ensure they receive the appropriate attention. Application logs are invaluable, not just for troubleshooting, but from a security standpoint. If you apply monolithic logging strategies to a microservice architecture, you'll quickly be disappointed and frustrated with the level of observability that you get into your system. The strategies we discuss can give you a holistic view of your distributed system, that provides valuable insights into the security position of its operations.


Service mesh
- [Instructor] To help manage the complexities of service-to-service communication inside of a microservice-based system, we can use a platform known as a service mesh. When using a service mesh, there are a few prerequisites. Your microservices must run in containers, with Docker pretty much being the standard these days. They must also be deployed in a container orchestration system like Kubernetes. You can find many vendors that package a container runtime in container orchestrator, or you can leverage the open source distributions of these platforms. Once you satisfy these prerequisites, you can use a security mesh to establish a network of proxies that sit next to the containers that host your microservices. These proxies, known as sidecars, intercept the traffic entering and exiting a service, providing an excellent point to apply security tactics, like mutual TLS, access policies, and audit logging. As more instances of a microservice are deployed, so are the proxies that sit alongside them. This begins to form an interconnected service mesh. The mesh is transparent to microservices because it runs a proxy in a container that shares the same pod as the container running our service. The fabric formed by the mesh delivers an excellent strategy for managing and controlling the operations of the microservice deployment by intercepting the traffic between them. If we inspect the high level architecture of Istio, a popular security mesh provided by Google, we'll find that it splits logically into a data plane and a control plane. The data plane contains the set of proxies that microservices use to communicate with each other inside of the service mesh. The policies and configuration of the proxies in the mesh is managed, configured, and deployed by the control plane. Using a component known as Pilot, the control plane is able to push these policies and configurations to the proxies inside the mesh without any code changes to the actual microservices. The policies can define authorization roles enforced by the proxies. These policies can range from very high level, such as which services are permitted to communicate with each other to very low level, such as requiring claims on a jot that is passed to a service. We can combine these authorization strategies with other security tactics to create a deep defense in-depth strategy for our microservices. In addition to receiving policies, the proxies can also extract information, such as traces or logs from the mesh into a logging platform. Istio's tracing capabilities increase the observability of our microservices. It allows them to be better monitored. Istio's tracing capabilities increase the observability of our microservices to allow them to be better monitored. Request tracing within Istio uses a quest ID and a trace ID header that allows individual requests to be monitored as they are flow through each service in the mesh. These headers can be extracted and used as a correlation ID in our microservice logs. Istio also has its own internal access logging that allows for detailed auditing of network transactions that can be visualized in order to see the path a request takes through our system. One of the major benefits of Istio is that it provides mutual TLS with just a little bit of configuration, right out of the box. Istio bundles a certificate authority named Citadel into the control plane that has extensive functionality. Citadel can mount the certificate and key pair into a Kubernetes pod when it's created. It's also capable of automating the rotation of certificates within the pods. Using Citadel, you get mutual TLS with only a little configuration, and that's a lot of value and security for your microservices. A service mesh in general provides a wealth of features that make management of service-to-service communication less daunting and more secure. If you're considering tactics, like mutual TLS, it's a great option to consider. Service meshes are also relatively new, so as innovations arise, they're likely to play a greater role in microservice architectures.


Application and container security
Throttling and rate limiting
- [Instructor] As an API gains adoption and its usage increases, it's important to maintain the experience and to meet the performance demands of its clients. Often, the operations found on an API are composed from those made available in a cluster of microservices. These microservices must remain stable as traffic increases. Scaling is the first strategy to consider when demand for a micro-service begins to exceed its capacity. By design, microservices are intended to scale easily through features like auto-scaling that are provided by container orchestrators. However, auto scaling has its limitations because constraints such as costs or available resources may limit the number of microservice instances that are able to be spun up. Underlying each container orchestrator is a plane of host VMs. Once their capacity is exhausted, there's nowhere for another container to be deployed. In some cases, such as a denial of service attack, you may not even want to scale. Seeing an increased demand for your API is great, but it may not be feasible to allow every client unrestricted consumption of it because it could bring down the API for all clients. Throttling the number of requests is one strategy used to control the amount of API usage by clients. It is based on the principle that it is better to deny some traffic to preserve the existing usage, than to crash the system and have no traffic at all. At a high level, you can think of throttling like a speed limit for APIs. The most basic strategy for throttling an API is to apply a universal cap on the concurrent number of requests made to a service for all clients. This cap is typically placed just below the maximum capacity for the service. There's problems with using a universal cap. It can allow one client to unfairly consume the majority of the capacity of the microservices. A better strategy is to create a quota and a rate limit for each API client. The quota defines the number of calls permitted by a client over a long period of time, like a day or a month. A rate limit focuses on preventing bursts of concurrent calls over a much shorter timeframe that could take down the microservice. In some cases, certain clients may have higher rate limits and quotas than other clients. For instance, a first party client may have a higher rate limit than a third party. For things like admin tools, we may not want to apply a throttle at all, since they're used to resolve issues. For APIs that are monetized, you'll see different tiers that are available, via an API plan that allows consumers to purchase more quota or an increased rate limit. Rate limits and quota can also be tracked and enforced at more granular levels. Restricting quota by end user can prevent a particular user from consuming the entirety of a client's quota. This allows available resources to be fairly assigned across all users of a client. When applying this strategy, the claims on a jot can be used to determine the identity of the end user and the client. Another effective throttling strategy is to apply a rate limit and quota for each particular operation on an API. API operations normally span multiple microservices. Each of these microservices may have their own resource profile with some consuming more memory and CPU than others. By having separate throttling limits applied for each operation, clients consume less resource intensive operations more frequently while heavier operations are made less available to prevent downtime. Availability often gets overlooked when it comes to security, especially when it's valid end users that bring down a service. Luckily, there are many techniques for throttling your API that you can choose from to keep your microservices available.


Container runtime security
- [Instructor] To receive the full benefits of a microservice architecture, it takes a flexible execution environment that allows for rapid yet stable deployments. This is why the majority of developers choose to deploy microservices inside of containers. And in most cases, that means Docker. Like each new technology in the stack, containers have their own attack surface and expose new attack vectors. When running microservices on containers, security measures can be applied to harden various components within the architecture, including the host and the container runtime. The host where the container engine is installed is a starting point for securing the container platform. It should have a very limited number of users that can access it directly. If you're using a container orchestrator like Kubernetes, this number should be even smaller, because the need to access the host is diminished. When choosing an operating system for the host, it's best to use a thin distro specifically designed for using containers, like Red Hat's Fedora CoreOS. These distros reduce the attack surface on the host by removing non-essential applications. This lowers the number of vulnerabilities found within the operating system. The host should also be hardened in accordance with the same security strategies taken to secure other VMs in your environment. When running containers, whether it be directly with the container runtime or preferably with the help of a container orchestrator, there are several important runtime security configurations that need to be applied. Since Docker is the defacto standard for containers, the remainder of our security discussion is going to be pretty specific to Docker. A good best practice is to configure containers to run with the least amount of privileges necessary. An effective way to achieve this is to drop all the kernel capabilities initially, and then add in only those capabilities that are necessary to run the container. This should be accompanied by the no-new-privileges flag, which prevents the process inside the container from gaining new privileges during execution. Another dangerous configuration option to be aware of is the privileged flag. This flag should be avoided at all cost. It grants a large amount of capabilities to the underlying Docker process running the container, which can be used to escape the container and launch attacks on the underlying host. It's also important to set the user that will be used when running the image. This is done either with the image configuration or when the container is launched. If a user is not set, your container will run as root by default, and root inside the container is root on the host. If an attacker manages to escape the container, they will have complete access to the underlying host. Finally, running containers using a read-only file system and volumes is a great way to reduce the blast radius of any containers that are compromised. These runtime configurations can help secure your Docker containers in a microservice system. If you're looking for a tool to help ensure your containers are securely configured, you can use Docker Bench to check for security misconfigurations. This tool will scan running containers and report on any configurations that deviate from benchmark industry security standards. To secure containerized microservices at runtime, you must account for the additional layers in the stack and understand the security implications of how they are configured. This responsibility may span across multiple positions in an organization. So it's important to standardize these best practices via automation. Once your containers are running securely, your microservices can truly enjoy the benefits of the flexible deployment and scaling without serious security risks.



Image security
- [Teacher] When securing microservices in containers, a lot of attention is paid to the runtime configuration. However, we need to keep in mind that securely building and sourcing images is just as important. Images are built from instructions found in a Dockerfile. These instructions add our microservice to the image and configure it to run the service. You can think of a Dockerfile almost like source code for an environment. Once the Dockerfile is ready, it is built into a container image which is similar to how developers compile the source code for an object into a class. When we are ready to run the image, an instance of it known as a container is created, and it is ran on the container engine. If you're a developer, this is like creating an instance of a class. Just like classes in object-oriented programming, you can use a class written by another developer, you can build your own or a combination of the two. Containers are the same way. Typically when developers build their own image, they start from one that is provided to them, which is known as the base image. Then for each instruction in their Dockerfile, a new layer is added to the image. Finally, when the image is ready, it is built and stored in the registry where it can be pulled and run as a container. Now, let's layer some security principles on top of all this. First, it's very important that the base image is an official base image provided by a trusted source through a trusted repository. Typically, you'll be getting these images from major technology players like Microsoft, Red Hat or official Linux distros. If you pull an image from an untrusted source, it is possible to introduce a malicious container into your environment that has been planted by an attacker. This is one form of a supply chain attack where malicious code is inserted into the build process, ultimately leading to a corrupt container running in production. Many organizations manage their own image registries, and only allow their container platforms to pull approved images from them. They only permit official trusted images into the repository and only allowed developers to add images through the CICD pipeline. Combined with effective security scanning, this strategy effectively seals where images can be sourced and introduced, helping to prevent vulnerable images from entering the environment. Now, just because an image was deemed secure at one point in time, doesn't mean a vulnerability can't be found against it. So here we're looking at a Red Hat Python image. And you'll notice that if we look at the latest version of the image, it is secure. Now, if we go back and just take a look at the image 18 days ago, you'll notice that a CVE was found against this image or a piece of software that it uses. Now, this isn't a knock on Red Hat, it's actually very good that they're keeping up-to-date with the vulnerabilities, but it does create the need for users of the image to follow it and make sure they're using the latest version. Most providers of official images will routinely update them to address critical vulnerabilities identified in CVEs. So it's important to follow them and stay up-to-date. To ensure you're using the latest secure images, the image registry should be scanned frequently using an automated toll like Snyk that detects potential vulnerabilities related to an image. This can help identify containers created without a date base images that have recently discovered CVEs and vulnerabilities against them. Typically, this is fixed by rebuilding the image with a new version of the base image, and then redeploying the container to replace the older instances that are running with the vulnerabilities. Another best security practice is to choose a thin base image, which limits the tolls it contains, because it helps to reduce the attack surface of the container and it can limit an attackers options, if they were to compromise a microservice. Technologies like containers place control of the entire stack into the hands of developers, who may unknowingly do the wrong thing. It is important to establish good security practices surrounding images that give developers the power to deliver new microservices while staying within some security guard rails. To achieve this, you may need to take a closer look at your toll chain to ensure it supports new technologies like containers and microservices. Many of these technologies have outpaced traditional security practices.


Audit container security
- [Instructor] When assessing the security postureof any component within a system,it's best to rely upon a tool to check for vulnerabilitiesand the enforcement of best practices.When it comes to running Docker containers,Docker Bench is one of the best tools for the job.Docker Bench is a small scriptthat inspects the container host to show how it complieswith the Center for Internet Securities Docker Benchmark.This is a formal list of security best practices for Docker.Now let's get hands-onand I'll show you how Docker bench works.So here in Windows subsystem for Linux,I've installed Docker on Abuntu.And then within Docker I have a single containerrunning our microservice.Now let's analyze the security posture of the hostand the container with Docker bench.The first thing that we'll do isclone the Docker bench repository so that we can get a copyof the script on our host.Alright, after the repository is cloned, we can go aheadand change directory into the repository,and then from there we will execute the script.The name of the script is Docker-Bench-Security.sh.Let's go ahead and kick that off.All right, it looks like Docker Bench has completedits security analysis.Let's take a quick look at the outputto see what type of findings it came up with.The output is arranged in sections that correspondwith the host, the Docker Damon, container images,and the running containers.Within the output you'll want to pay close attentionto the red warning tagsbecause these are here to alert youto a potential security misconfigurationin your container platform.Let's take a look at the warningsin the container runtime sectionwhere Docker Bench has analyzed the security postureof the running containers.It's down here in section five.You'll notice that within this section we have severalof these red warnings.Let's just take a peek at a few of themthat illustrate the best practices we spoke aboutin our last lesson.For example, in finding 5.13, Docker Bench is warning usthat our container is not runningon a read-only file system.This was one of those best practices we talked about.We can fix this by starting the containerwith the read-only flag.And then here we can see finding 5.26,which tells us that the container isn't restrictedfrom acquiring new privileges.We can fix this by running the containerwith the no-new-privileges flag.So you get the idea here with Docker bench.You run the automated analysisand then inspect the findings, looking for opportunitiesto harden the containers based on the findings it shows you.I highly recommend that you consider running Docker Bench.It's a great way to improve the security postureof your microservices.

Secrets management
- [Instructor] Microservices often need to store secrets. Things like client credentials, database passwords, and SSL certificates that must be kept confidential. Unfortunately, many developers take a trust the network or security through obscurity approach to protecting secrets. This causes secrets to wind up in difficult but not impossible to reach places. You'll find them in properties, files, hard-coded strings docker files, container images, and environment variables. If you use these techniques, your microservices secrets won't be so secret anymore with a little bit of access to source control in image registry or host. Luckily, container orchestration platforms like Kubernetes, OpenShift and others provide better approaches to managing your secrets. Now there's no excuse to bake them into a container image. These platforms typically have an embedded secrets management capability that allows you to create and securely store, secrets like credentials or certificates for use in a pod. A pod is just a Kubernetes concept that is kind of like a group of containers. When attaching secrets, you'll typically have two options. You can inject them as an environment variable, or, you can mount them to the container using a file written in a volume. Writing a file to a volume is the most secure route. So I would recommend going that approach. If you're using a container orchestrator. An orchestrator is an easy way to handle secrets because it's built right into the platform. However, using this approach you cannot share secrets outside of a microservice cluster. This leads many organizations to adopt a solution for secrets management from a vendor product or their cloud host. Let's take a look at a popular option named Vault. Vault is secrets management platform provided by HashiCorp. It provides several strategies for microservices to securely store and access all sorts of secrets, including API keys, passwords, or certificates. To allow its clients, like microservices, to access secrets, Vault provides an API that can be used to easily obtain them. Another configuration model provided by vault is the Vault Agent. The agent provides tight integration with container orchestrators like Kubernetes by using a Sidecar model to provide microservices with transparent access to secrets. The Vault Agent handles the initial authentication with the API using an init container. It also allows templates to be created for microservice configuration files that have secrets injected into them. This approach eliminates the need to write code that retrieves the secrets from the vault API using HTTP calls. Dynamic Secrets are another advanced capability provided by Vault that allows credentials like those used for a database, to rotate transparently inside the microservice. This increases the security of the system because a stolen credential has a short life span. Another type of dynamic secret known as a lease, issues new short-lived credentials each time a client requests access to a protected resource. Managing credentials can be incredibly insecure using historical practices that are not suited for dynamic environments like containers. When supplying secrets to your microservices, I highly recommend considering a third party secrets management platform or, at a minimum using the storage provided by your container orchestrator.


Secure pipelines
- [Instructor] DevOps pipelines are a key enabler to achieving the rapid delivery cadence that draws most development teams towards microservices. If you think of a pipeline as a function, it accepts commits from a developer as inputs. And in the ideal scenario, outputs a container running the software the commit was made against. A lot happens between these two points. First, the code needs to be written and committed, which triggers the CI portion of the pipeline, where the software is built and tested. Then it's packaged into a container image stored in the artifact repository. When the CD portion of the pipeline is invoked, it pulls the container out of the repository and deploys it onto the container runtime using an orchestrator. In this flow, there is ample opportunity to inject automated security controls into the process. These controls act as gates that prevent vulnerable code or images from ever being deployed. As a strategy, it's best to detect and correct security vulnerabilities as early as possible in this process to prevent them from traveling further downstream towards production. Starting with the actual development, static code analysis tools that run on developer workstations can provide the first line of defense. These tools alert developers to security issues before they even reached the code repository. Even with scans in place on developer workstations, you should still run static code analysis on the CI build and fail the build for code that doesn't live up to the security standards. Access to the source code repository should be limited to actual contributors and there should be a pull request model in place that restricts who can commit code to release branches. Some tools like sneak allow you to incorporate security scans directly into your pull requests, providing direct feedback before the code hits the repository. When a microservice is built, all third party libraries should be sourced from a trusted artifact repository that provides a central point to access approved libraries and container images. After the microservice is built into an artifact, a barrage of unit and integration tests are typically ran against it. This provides an excellent point to inject some interactive security testing. These tests detect security issues while the code executes and alerts you to them. This provider it's additional feedback that can help the tech vulnerabilities. If all of the tests pass, the microservice is built into a container image that is placed in the artifact repository. Here, a registry scanner, which has provided as a capability in most artifact repositories can be used to detect new vulnerabilities that have arisen since the container was created. Access to the artifact repository should also be closely guarded. This prevents anyone from going around the security gates that are enforced by the pipeline. After the CI pipeline finishes, the CD pipeline triggers the container orchestrator to deploy the microservice images to a lower environment for further testing. This process should have strict controls, regarding the artifact version that is released. It should ensure that the artifact has been vetted through the earlier security gates. The same control should be applied once again, when an artifact is about to be released to production. In lower environments, the microservice can be subjected to dynamic testing that actively probes the service for a slew of well-known vulnerabilities. Typically, this is done through automated pen testing where a tole scans and probes against the microservice. These tools can integrate with the pipeline to gate any vulnerable images from advancing to production. Pipelines have many configurations. So we've boiled our discussion down to some common points within these platforms where security gates can be introduced to better secure your microservices. It is important to hone these automated security scans to focus on critical vulnerabilities because they can overwhelm development teams with false positives that create alert fatigue. Baking these security tests into your pipeline can help keep up your release velocity without compromising on security.


Prevent leaked credentials
- [Narrator] One of the best waysto secure your microservicesis to add automated verifications into a CI/CD pipelinewith static security analysis tools.To show you how these tools can provide immediate security,we're going to walk through a pull request workflowusing Azure DevOps,where I've enabled a credential scanner in my pipeline.So I have this change sitting in Git that I just finished.This change updates the password that our microservice usesfor its database connection.Unfortunately, we have to rotate this passwordevery 90 days, so we need this change for our microserviceto continue pulling data from the database.Everything looks good here locally,so I'm going to go ahead and push this change upto Azure DevOps so that I can create a pull request.All right, let's head over to Azure DevOpsand we can get to work on that pull request.All right, here we can see that my changes were detectedand I can click on the Create a pull request button,which will allow me to easily submit this pull requestfor the repository.Now once the pull request is submitted,you'll notice that we have configured a build pipelineto be kicked off automatically,and this pipeline is going to run various checksto ensure the quality and security of the codethat we've just pushed.Let's go take a look at the pipeline and see what it finds.Yikes, it looks like the pipelines failedfor our pull request.Let's dive in and see what happened.Here we can seethat our credential scanner has flagged the credentialthat we submitted in our change.That's because, as a security best practice,secrets shouldn't be checked into a Git repository.So you can see how automation is a very powerful toolfor securing microservices.A credential scanner is just one exampleof the many SaaS toolsthat you can incorporate into your pipelinesfor better security.Now let's move on to our next lesson,and I'll show you how we can securelyhandle this credential required for our microservice.

Keep secrets with Key Vault
- [Instructor] In this lesson, we're going to get hands-onwith Azure Key Vault to learn how to securely handlethe secrets required by our microservice.So you'll recall this connection stringfrom our previous lesson that we use to establisha database connection from our microservice.Now the challenge we face is how to securely storeand access this connection string from our microservice.For that task we're going to use Azure Key Vaultand the Azure SDKs.So let's begin by removing the secretfrom our configuration, and then we're going to navigateto the Azure portal where we'll create a key vault resource.To start that process click on Create a Resource,and then come down here to the Create link below Key Vault.This is going to open up a form where we will providedifferent details about the key vault resourcethat we're going to create.The first thing we'll do is select a resource groupwhere this key vault is going to live.Then we need to provide a name for our new key vault.I'm going to call my key vault "chatmicroservice-vault."Okay.And then you'll want to select a region that is closeto you or where your service operates.And then after that, just leave all the other valuesset to their default settings.All right, now we can click on Review and Create.And then once our resource details are reviewed,we can go ahead and create the key vault.This is going to start the provisioning of the resource,and soon we'll have a place to store our secrets.All right, it looks like our key vaulthas finished provisioning.Let's go to the resource where we'll needto put some additional configurations in place.The first configuration that we'll needto establish is RBAC.So we need to give ourself role-based access controlpermissions in order to be ableto create secrets within the vault.So to do that, click on the Access Control blade,and then come up here to Add. and then Add Role Assignment.This is going to present us with a list of rulesthat we can choose from to assign to a user.In our case, we're going to selectthe Key Vault Secrets Officer role,and then we'll come over here to the Members tab,and we're going to click on Select Members,and then we're going to find our accountso that we can be assigned the permissions.So once you find your account, click on it,and then hit Select,and then come down here and click on Review and Assign,and then click on it one more timeto create the role assignment.All right, now that that permission is in placewe can create our secret.And that's done by clicking on this Secrets tab,and then coming up here to Generate and Import.All right, the first thing we'll needto do is name our secret.I'm going to call mine the dbConnectionString.And then we have to provide a value for our secret.Here's where we're going to place that connection stringthat we copied from the configuration.All right, once that's in place we can go aheadand create the secret within our key vault.There you can see it in our list of secrets.Let's just take a quick look at this.You'll notice that the secret can be versioned.And then if we come into the Secret page,we can actually see the secret valueor we can copy it to our clipboard if needed.With our key vault established,we can now turn our attention to retrieving the secretfrom Key Vault using its SDK.So here in the NuGet packages for our microservice,you can see that I've includedthe Azure.Security.KeyVault.Secrets packageas well as the Azure.Identity packages.These NuGet packages will give us some typesthat we can work with to securely retrievethose secrets from the key vault.We can see the code that works with those typesin the ChatDbContext class.This is an entity framework classthat sets up our database connection.And here you can see that we've overriddenthe OnConfiguring method.Within that method, we use the secrets SDKin order to create a SecretClient that is pointingat the new key vault that we just created.Now, in order to log in and authenticate to get the secretsfrom the key vault, we use a default Azure credential.And this is going to source an access tokenusing the identity that we logged into Visual Studio with.So in this case, it's my account within Azure.Now, once we have the SecretClient,we're going to use this GetSecret call to fetchthat dbConnectionString secretthat we just created within the vault.Once we have that connection string value,we pass it into Entity Framework in orderto set up our connection to the SQL Server database.All right, let's go ahead and launch the appand then we'll head over to Insomnia to test out this call.All right, I can see that our microservice is up,and I've already obtained an access tokenwithin Insomnia that we can pass alongfor authorization when retrieving our chat messages.So all I need to do is just send the request.And once we send the requestwe should see chat messages returned if our integrationwith Azure Key Vault is established correctly.And there we do see our messages.So that's a look at how to store and retrievesecrets from Key Vault.It's slightly more involved than just placingthe secret right in the configuration.However, it's a much more secure approachfor handling secrets within our microservices,and the SDKs help us out a lot here.


Conclusion 

Securing microservices
- [Kevin] Thanks for watching this course on microservice security. In the course I've provided you with strategies for securing microservices that overcome the security challenges of a distributed system by using leading standards and platforms. Before we wrap up the course I want to provide you with some recommendations for what to explore next and give you some information about my other courses. If you want to get to the expert level I highly recommend reading the specifications discussed in this course, such as those for OAuth 2 and JWT. These materials will deepen your understanding of the standards used to secure microservices and allow you to teach them to others. Throughout my career I've learned the most through hands-on prototyping. I encourage you to download or sign up for some of the platforms mentioned in this course and to build small proof of concepts against them with the techniques we discussed. Prototypes pay huge dividends. If you've liked my teaching style I have a few other courses on the LinkedIn Learning platform that might interest you. "Git for Teams" is a course I built that deliver strategies for teams that use Git. It can make your team's development much more fluid. "Building APIs with Swagger" covers a design first approach to creating APIs with the OpenAPI specification. "Java EE 8 Essential Training" covers 13 of the most widely used Java EE APIs and gives you a rock solid foundation for building Java web applications. "Spring Batch" can provide you with a proven architecture to construct complex batch jobs that run on the JVM using Spring. "Migrating beyond Java 8" will teach you how to navigate the Java ecosystem to handle the new release cadence and newly introduced language features. So that's all I have for you for now. Congratulations on finishing this course and thanks again for taking it. Now put your new skills to the test and best of luck securing your microservices.