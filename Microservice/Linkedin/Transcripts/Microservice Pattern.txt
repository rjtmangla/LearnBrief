https://www.linkedin.com/learning/microservices-design-patterns-23454771/solving-microservices-problems-with-patterns?contextUrn=urn%3Ali%3AlyndaLearningPath%3A645bcd56498e6459e79b3c71&u=0
Microservices: Design Patterns

Solving microservices problems with patterns

- As you venture into the world of microservices, one thing becomes very clear. The problems that you encounter in building, converting, and operating microservices are not unique just to you. As with all of software engineering, we as a collective create design patterns so we can fix the problem once and reuse that pattern every time we encounter it. Hi, my name is Frank Moley. I'm a cloud native developer, an architect, a teacher, and a perpetual student. Using the patterns we will introduce in this course will save you time and energy and allow you to focus on your business needs. So let's jump in.


Vernacular of microservices

- Before we get too deep into the world of microservices design patterns, I want to level set on the vernacular that I will use when discussing microservices. These are not authoritative, these are just the terminology I learned and grew up on. The first set of terms that I want to define are service types. These will be critical to understanding how various patterns are implemented. A data service is that which connects to a data source within the system. This does not mean databases alone, because any valid source that can be served through a microservice applies. Data services are usually bound by domains defined within the global architecture. A business service or business process service is a higher level of abstraction that builds on data services. Often we need to define business domains that transcend individual data services in order to be correct from the business perspective. A translation service is any abstraction on a third-party operation that you want to encapsulate under your own facade. An edge service is responsible for serving data to users and external systems. These services can be used to provide a web view, a service that delivers the content, and maybe a service that delivers to mobile devices. When defining a platform, there are many different things people can think of. For the perspective of this course, I will consider the platform as the all encompassing arena for all service operations across multiple data centers. The platform can include the infrastructure, runtime, ancillary services, networking, storage, and the list goes on. If I have learned anything, especially as I have built more publicly visible systems, is that operational components should never be overlooked as part of a platform, and neither should diagnostic components. While these terms are not definitive, they are the terms I will use in this course when talking about microservices patterns.




Microservices and cloud native

- Let me start by saying one thing very clearly, microservices do not make a system cloud native. And cloud native doesn't require microservices. All too often, these topics get merged, and I want to be very clear from the beginning about this. Now, from my perspective, and from the perspective of this course, we're going to focus more on how to utilize microservices in a cloud native pattern for building our system. "Why?" you may ask. Because cloud native can literally run anywhere, not just the cloud. So let's talk a little bit about what cloud native means. First of all, cloud native is an architectural style. What that means is that it isn't a pattern per se, of how to solve a problem. Instead, it's a way of doing processes and building systems to facilitate an end goal. Cloud native is designed primarily to facilitate operating in the cloud. It includes things like externalizing configuration, focusing on scalability, making your application startup fast and handle immediate shutdowns gracefully. There's a whole plethora of ways to represent cloud native applications. I have a course here in the library called "Twelve-Factor and Fifteen-Factor Cloud Native Application Development" that I encourage you to look at when wanting to learn more about cloud native. Cloud native applications are designed to be portable and scalable. Portability comes into play when you can deploy or move your applications anywhere globally, and they still work without additional code. Scalability entails building your applications to run as a single unit or multiple units depending on your overall needs of your system. Advanced designs like using autoscaling to allow your services to scale up or down based on the overall system load, increase the need for this scalability. It is very important to note that there is nothing that prevents you from building cloud native applications and running them in your corporate single instance data center. So now let's talk about microservices since that's why you're here. I want to start by saying that this is very open to personal interpretation. There is no clear definition of what makes a microservice micro. I have seen nano services that go overboard, and I've seen smaller monoliths that don't go far enough. For now, we will just talk about microservices being smaller, scoped units of work. Monoliths usually expose many applications and services in a single artifact. Scaling these systems was a massive undertaking. Microservices are all about breaking those endpoints up into distinct units of work so we can scale them independently. We often hear these concepts talked about at the same time. Both cloud native development practices and microservices architectures put an emphasis on building scalability into your application and system as a whole. Now, often when building cloud native systems, microservices appear to be the clear path to get there, and this can muddy the water some when talking about these two topics. It is important to note that you can do cloud native development without building microservices, and likewise, you can build microservices, not targeting cloud native deployments. It is important to make sure that this is clear. All too often I hear people talking about one topic and implying the other. Even though they're so distinct, they work so well together to solve the common problem. So please just take into account that even though some may consider them requirements of each other, that is not the truth in any way, shape, or form.



Decomposition Patterns
Decomposition of a system

- We have talked about microservices being smaller, but what does that really mean? Consider for a moment how you decompose a problem when writing a piece of software. You don't cram everything into a single function and then just finish. You break the problem into logical steps and convert those steps into reusable functions that can be used as the code base grows. This is much the same with microservices, but instead of a single problem statement, you are looking at all of the problems across the system as a whole. The first area of decomposition we will talk about is the service types that you can create. These are designed to provide very specific uses across the system as a whole. Each service has a distinct function in the system, and through building these services, we decompose the problems that we need to solve into smaller blocks of work. Domain-based services are usually one of the first areas we attack when building microservices. We will discuss how we use domain-driven design to build functional domains and then break up our system on these domain boundaries. Another breaking point in service designs come from your business processes themselves. Sometimes complex processes have so much inherent coupling that you cannot further break them down into smaller domains while still maintaining sustainable traffic patterns across your system. Because of the needs of network health, these business process domains become a clear point of delineation within your system without further breaking them down. Now, in some business models, eventual consistency isn't an acceptable model. Well, these are few and far between for most industries, however, there are some use cases for this, especially in areas like the financial services realm. As such, business processes themselves may be too broad to still achieve an atomic state, so you need to build your decomposition model around the atomic transaction itself. Functional use patterns are an important point that we will dig into some, but we need to talk about ways to achieve this in a microservices architecture. The first one that we will discuss is a very powerful strategy to migrate from a monolithic system into a microservices architecture. And this is called the strangler pattern. It is by far the most common pattern talked about when dealing with microservices because often, we are breaking down monoliths instead of writing new systems. Another strategy that we will discuss that often gets used to promote separation of concerns across the system is the sidecar pattern. This pattern allows us to offload many operational functions into separate components that they themselves get deployed alongside the main service component so that we don't litter our code with all of these functional needs across the system. So now that we have an overview of decomposition patterns, let's jump into our domain-based services.


Domain-based microservices

- Domain based microservices, as you might imagine, are based on domain driven design patterns, but what does this mean in practice to somebody building out a services architecture? Data domain decomposition is the lowest level of decomposition that you will usually see in a microservices architecture. The core problem we're trying to solve with this is to make our services more scalable. As such, you need them smaller and much more focused. Decomposition of the domain level is one of the most efficient ways to do that. These services are driven by the data domain itself. They focus on serving the data as it will be used throughout the system and applying logic for the domain itself while ignoring all logic outside the domain. And that's the key. Only worry about the logic within the domain itself. The underlying schema is not important when designing these services. The access patterns for the data domains is what is important, even though the two may share similar features. The key factor is that they focus on the data patterns and the domain itself. You will build decomposed services that focus on one data domain. Sometimes, a domain shares enough functionality with another domain that it may trigger you to merge them into a new single domain. Sometimes, you'll identify areas where you thought concepts belonged in a single domain, but there isn't enough sharing and you'll need to split them up. But again, the key here is to focus on your data and how it is actually used throughout your system. When it comes to designing your domain, and hence your domain services, there's not a recipe card that can make this happen. Much of the design comes from trial and error with fast iteration, but experience will reduce this burden. Telemetry is your biggest friend here. It should provide you more than experience will in that you will have solid data for decisions. Hopefully, it is clear that we should be doing domain driven design. This course isn't going to be one on domain driven design however, as that is at least one course on its own. Now for the steps. You need to start with the domain model when building services around data domains, not the database schema. The model itself should represent your domain to the outside world, and you need to consider how the model is represented, utilized, and consumed. Next, we need to evaluate the actions that we'll need to perform on these models. Don't think solely in terms of CRUD and don't think solely in terms of REST APIs either. You can work actions into the API if they are applicable to your domain, and many times, these actions are where the real work is getting done. This should yield service definition itself, the API contract. Again, we're focusing on the model and not the underlying data store. Then we work our way down to ultimately the storage of the data and implementation of the actions themselves. Let's take a look at a visual representation of how we would build these services. First, we start with our model design. This includes all of the attributes of our model itself. Next, we define our actions. Now, you're going to then write your service boundary and expose your actions as APIs. Finally, you build your data store to store the data or build the functions and translations to utilize the existing data store. Note, the model doesn't have to match the schema in the data store itself. Now this is a very simplified view of the pattern because it's all based on domain driven design. Of all the patterns, however, this is the one that you'll use the most.


Business process-based microservices

- [Instructor] Oftentimes when building out decompositions, you run into more complex processes. Often these processes don't fit into a single domain, but the domains themselves can live in a standalone fashion. Enter into the picture business process-based microservices. A business process service can help you build a more structured microservices architecture. The problem that you may be faced with is that in order to replicate business processes across different components, you have to recreate the same code logic multiple times, especially when they span multiple domains. In order to adhere to the don't repeat yourself principle and to help simplify your architecture, you can leverage business process domains as a higher level of service within your architecture. First and foremost, it provides a higher level of abstraction written around specific business functionality. This helps you not only encapsulate these use cases, but also scale them more effectively in your system. It also provides a mechanism to encapsulate your related domains. A warning here. Building extra layers for uniformity, for instance, only increases your network traffic unnecessarily. One key aspect in business process services is that they usually don't have their own data access. This allows you to not muddy the waters between business domains and data domains. While this pattern should be considered, many do ignore it. From my experience however, ignoring this will bring you a lot of headaches going forward if you start merging the two concepts. It is better to keep your boundaries tight and work within the overall architectural model. The final key aspect is that they should have distinct functional uses. You don't want to build out all of your business processes into a single domain package. If they are related, especially in use, it's fine to put them together, but be very careful you don't layer a monolith into your system with business processes. Building a business process is fairly straightforward. You start by identifying the process with which you want to expose. This may sound simple, but oftentimes, the rules are very specific when it comes to these business processes. As such, you need to spend some time here gathering requirements and ensuring you have the functional needs defined before you start. Once you have the business process defined, you should be able to identify the data domains that you will need to consume. Next, you need to define the APIs that handle your business processes. Once again, we focus on the contract and not the underlying models. The API should represent the actual processes that are needed, and many times, you may find that the action pattern for RESTful services is very useful in this level of abstraction. Finally, we will build our service and wire the API to the client calls for the domain services. It is recommended that you actually encapsulate your business process code into its own module for easier maintenance. Let's take a look real quick at a visual representation of the design strategy around building business process services. First and foremost, we define our processes themselves. In this case, our service will expose three related, but different processes. Now we can identify the domain or domains that these business processes need to interact with. In this case, we have two unique data domains. Next, we define the APIs for our business processes. In this case, we have one API for each. Finally, we will implement and wire our service together using client code to interact with our data domains. The implementation should be a black box to the outside world. Consider using internal modules and solid encapsulation for each process or even step in the process as this will help you iterate as the process changes over time.


Atomic transaction-based microservices

- There are potentially times in a microservices architecture where you need to have true atomic transactions because eventual consistency isn't good enough. When those atomic transactions span multiple data domains, there is a distinct need to build special logic and systems to handle these unique use cases. One of the primary characteristics of these services is the need to guarantee ACID-compliant transactions across more than one data domain. If you need atomic transactions in a single domain, there is no need to specialize a service, because the underlying implementation is hidden. However, providing these cross-domain services that support failure domains and rollbacks of the entire domain scope must have a solution. These services force a blocking API call until the commit is successful. While you can logically do these asynchronously, usually the caller needs a guarantee of success or an error, so the API must be synchronous and blocking. You may be tempted to use data domains with distributed transactions. I would highly suggest you stop and don't do this because of the complexity that it adds to your system. Designing these atomic services is very straightforward, but you need to do some homework first. Ensure that you really must have the service be atomic. While many times people say that they need atomic transactions, the cases of actually needing them are fewer and further between. Spend some time and make sure that your stakeholders understand the complexity as well as the constraints that this puts on your system as a whole. But in the end, you may really need them. When building such a service, you start by ensuring that your underlying data domains share a database. If they don't, you need to merge them into a single shared database in order to build your atomic service. This risk also needs to be understood by all involved because now you're modifying existing functionality. You'll then need to get clear and concise rules defined about the transaction. Because this will be a blocking call, make sure everything, including rollback conditions, are well-defined. Consider documenting these rules in code comments or readme files for future reference. Finally, you need to implement the service with its rollback handlers. Rollbacks take time, especially in a blocking call, and those confirmations to remote systems can become bottlenecks. You must exit the function with a complete commit or a complete rollback. A final word here on these. If you can avoid them, please do. They muddy the water for distributed systems as a whole. You cannot adequately distribute the databases as effectively, nor avoid the potential network congestion. Try to sell your stakeholders on eventual consistency or simulated rollbacks, of course, explaining their risks as well. Atomic process services are hard to implement, but when they're needed, you must have a solution. I encourage you to collect as much data as possible to ensure you are making the best decisions.


Strangler pattern

- Now that we have discussed some of the most common decomposition patterns, we need to talk for a bit about how to get there from a legacy system. So let's talk about the strangler pattern from a high level conceptual point of view. It really is a simple pattern as long as you can plan appropriately. The idea behind the pattern as a whole is you start with your monolith and then essentially carve your dependencies off of it. You shard your services off piece by piece into new microservice endpoints. Point the internal client to the new microservice, and eventually, deprecate the functionality out of your monolith once all of the dependencies are weaned off of it, and you've now strangled it. You can break your services off at the API layer first by building out the API and underlying service, and then moving the database behind it, assuming it's a data domain service. You can likewise start at the data store itself and move the domains out, and then traverse up the tree to business processes and so on, depending on which part of your system is causing the most pain. Knowledge of this will help you determine which of these two strategies to employ. Essentially, both of the same. You're carving functionality out of the monolith and replacing it with a properly designed microservice artifact within your system. You move the processing to the new microservice, and eventually, your monolith is strangled away. Okay, so let's look at the bottom-up type of strangling a monolith. Now, this, of course, is going to be a very simple example, but we're going to start with our monolith itself. It has a couple of clients and a database. Now, within our monolith, we identify two business processes within it. We then define three distinct data access areas that can be defined as data domains. In the bottoms-up approach, we'll use a single service database design, something we're going to focus on later, but for now, just know that each domain gets its own database, but we don't want to do it all at once. The first thing we need to do is we need to build a new database. Next, we need to build our data domain. Of course, at some point here, we will need to sync the data and keep it in sync, but that's beyond the scope of this course. Now, we move our client over to use this new data domain. Once all the data is moved over and the usage from all clients are gone from the monolith, we can then remove the data access block and data from our original database, as well as syncing logic that we have put in place. The same pattern can be applied to different service types up to stack. Eventually, with this pattern, you will strangle the monolith completely from being consumed from clients, and at that point, the monolith itself can be deprecated.


Sidecar pattern

- The sidecar pattern is used to offload processing of some kind to a separate module. With the sidecar pattern, you deploy the sidecar as a module associated with every applicable microservice in your system in order to offload the processing through a single code path in an isolated process. The sidecar pattern is more about removing repetitive code across services than anything else. The reason that it is helpful in a microservices architecture is things like logging, monitoring, and network services can be offloaded to a separate module. So essentially, the problem we are solving is repeating code paths that don't have to be accomplished through repeating code. The details of this design are actually very straightforward. You start with the process itself. The key here is you need to write the module specific enough for your immediate needs while being generic enough for other parts of your system. This really is the hardest part of sidecars because you have to be willing to get it wrong before you get it right. Once the design is in place, you simply write your code. Most runtimes that you would use with the microservices architecture have sidecar processes or strategies depending on how they containerize or provide access to that runtime. The next step is simply to schedule your sidecar to be deployed with the appropriate service. Most of the time, this is accomplished via the parent service manifest or service definition. Once you deploy it, the functionality should just appear. You never have to actually modify the code of your main application service, and that's really where the power is. Because as we implement one piece of functionality, if that piece of functionality now needs to live somewhere else as well, we simply upload the sidecar with that element and it will inherit the same functionality. So let's take a look here at a quick visualization of the sidecar pattern itself. We're going to start with three microservices sitting in our virtual network. Now we add our first sidecar, and in this case, it's a logging sidecar to each service. Note how the representation here is very focused on containerized deployments as this is usually where sidecars are most utilized. We actually are deploying these sidecar elements to the parent process itself, but we're getting the benefits of the sidecar everywhere while not writing code into each service. Now we're going to go ahead and add another sidecar. This time, a monitoring one. Again, applying it to each service, and once again, we are uploading it as part of the parent service when we do the deployment. So all of these run as a single artifact, though they are very distinct processes. Now the last step is that we're going to add a security sidecar, but this time, we're only going to apply it to one of our services. Again, another power behind sidecars. You pick and choose where you want to apply the functionality and simply apply it. As long as your sidecar is written generic enough, you can apply it anywhere, and that service will automatically inherit the functionality. Now as I scale each one of these services, the sidecar comes along for the ride for free and we inherit that functionality once again. This sidecar pattern reduces duplicate code, provides unified and focused behavior, and does so natively as part of the natural deployment process.


Integration Patterns
Gateway pattern

- We will now move from decomposition patterns into integration patterns. These patterns allow you to solve orchestration and ingress needs across your system as a whole. The first pattern that we're going to talk about is the gateway pattern. The API gateway pattern or gateway pattern is an ingress pattern for clients communicating with your system services. The problem statement we're trying to solve is that of chaos. If we allow any client of any system to access any service however they wish, operations and maintenance needs will skyrocket across the system as a whole. This grows even more chaotic as your client set increases, especially if third-party vendors start consuming your APIs. The gateway pattern is designed to provide a buffer between the underlying services and the client needs. That can be accomplished via facade or a simple proxy, each having risks and rewards. This single layer becomes the interface for the outside world. This can include client systems that you own. It can simply proxy the calls to your underlying services. It can mutate the calls, or it can limit the calls based on what the gateway itself exposes. This also, however, can become a single point of failure for a system, so care needs to be taken to ensure that it scales and responds well when the need arises. One of the most powerful aspects of a gateway is the ability to mutate the payloads from your system in a specific way, customized to the consuming client's needs. In its most simple form, it can proxy the backend calls. Using a gateway in this way allows you to put your security and authorization logic in a single ingress point. You can restrict or allow access based on the client and its credentials. You can have a simple buffer so your internal system doesn't have to be exposed publicly. Only the gateway itself needs public ingress. The gateway can also decorate payloads. Oftentimes, we need to have common headers or other data points in the payload that are not necessarily relevant to the underlying service. By using the gateway, we can apply that decoration in one place and in a consistent way. Another powerful aspect is that we can do aggregations. We have many use cases like high bandwidth desktop clients that can and should consume more data at one time, so we can use the gateway to aggregate the payloads under these approved situations. The gateway makes the underlying calls and assembles the data into a single payload for the client. Be very careful here of not applying business logic when doing this. It can and will lead to an unmanageable gateway. Do only simple aggregations. For more complex rules, please stay tuned for the process aggregator pattern. In a similar vein, you can limit access. Say for instance, you have a lower bandwidth mobile client that doesn't have the screen real estate, let alone the bandwidth to handle a full call to your underlying API. The gateway allows you to limit the data you send to these clients while not modifying the underlying services or the calls for other clients. One of my favorite benefits of this pattern is it gives me the ability to provide installation. Consider having web, desktop, and mobile clients of your system as well as public API clients that you manage and distribute. The gateway pattern gives you a contract-driven API point. They can be static while the underlying services can change, migrate, and evolve as needed. The gateway implementation may change, but your clients don't need to feel the pain of that change because they adhere to your public contract. The strategy for building a gateway is actually very straightforward. First, you need to define your contracts. You can start small here. Remember, however, that this is your public touchpoint. Now, you expose these APIs in your gateway component. These APIs are static, or at least passive contracts, and can be tailored for specific clients. You want to keep the contracts solid, so you may build out sets of APIs for each client independently of the rest, or just focus on the most dynamic APIs and expose them as being client-specific along with a core set of global APIs. If you haven't noticed, I've mentioned passive a few times on purpose. These APIs are your public touchpoint. Nothing, and I mean, nothing makes a developer more upset than having to fix code that worked yesterday because someone in a third-party system broke the passivity of a contract. In order to keep these contracts static, you use strict version control, like semantic versioning, to keep your APIs growing, but to help ensure passing behavior. Of course, the versioning isn't enough. You have to make sure your changes are passive as well based on that versioning strategy. Now, you simply implement your gateway. It is recommended that you build your internal client code indistinct modules and then consume these modules within your implementation. By doing this, you could isolate the changes that are needed on the backend to support the growth of your internal APIs as a whole while keeping that public contract rock solid.


Process aggregator pattern

- The next integration pattern that we will discuss is an orchestration pattern called the process aggregator pattern. The process aggregator is a very straightforward way within your system to develop complex processes. When we have multiple data domains that need to be called together within a business process, we use a business process service. There may be cases, hopefully and frequently, where you need to do the same for business processes themselves. This is where this pattern comes into play. So the problem that we need to solve is we have built out business processes within our system, but we can see a frequent need to call two or more of them at the same time, in certain use cases, and then build a composite response from it. This is where this pattern shines. The aggregator provides clients with a single API call. This API contract not only handles the underlying business process calls but assembles the payload for the client system. The process aggregator can, and really should, introduce its own processing logic. If you are building an aggregator to simply group calls for your clients, you can leverage a gateway aggregator or even just keep the calls in the client. The aggregator really shines from a value perspective when it is also encapsulating the business logic for these aggregation calls. If every client has to reproduce a set of rules when calling multiple business processes, you're breaking the "don't repeat yourself" or DRY principle. The aggregator encapsulates that logic and then has the added benefit of consolidating the payload for the client for the parts of the model that are relevant for the aggregator call in the first place. I will urge you to please use this pattern with caution as it can introduce a choke point in your system. Consider the case of a loan process while there may be several business processes as well as multiple data domains in play. In addition, in order to proceed from one process to the next, you need to complete the first business process call. This can make your aggregation call a very long-running process, which in turn is a blocking call for the API. This choke point can bring a distributed system to a crawl as network IO increases and wait times also increase. Once you get into this realm of large data processing operations, you really need to consider asynchronous patterns instead of aggregators. Designing and building an aggregator should look very familiar at this point. The first thing that you need to do is determine which business processes are needed behind the aggregator. Ensuring you know the business processes first will help you make informed design decisions going forward. Now, you then need to determine the processing rules. Again, once you understand the process and the processing rules, you can make better-informed decisions going forward. Now, based on the previous steps, you should be able to define a new model for your aggregator. This really shouldn't be a simple pass-through and consolidation of your underlying processes because your internal processing rules will add or modify that model in most use cases. Once you have your model, you can implement the API based on that model. Many times, if you design that model right, you can use standard REST verbs on these models. But if not, you always have the action pattern as needed. And finally, you need to wire the service together and implement the internal processing features. Once again, and I may sound a little bit like a broken record here, I urge you to encapsulate your internal business processing rules to prevent a risk of changes down the road causing more problems. This encapsulation helps with design as well as maintainability of your system. The aggregator pattern is useful in some cases and is a good pattern to have in your toolbox. Just heed the warnings about the pattern before you jump in. Abstraction for the sake of abstraction does you no good in a distributed microservices architecture. And if you find yourself going down that path, I can almost promise you you will see network issues in the future.



Edge pattern

- The final integration pattern that we will talk about in this course is another ingress pattern called the edge pattern. Much like the aggregator was a subset of the gateway pattern, the edge pattern is also a subset of the gateway pattern. The basic problem that we need to solve with the edge pattern comes into flavors. The most common is that using a gateway becomes a scalability concern as one client type, say, mobile, contributes to request volume significantly more than other clients. As such, scaling the gateway becomes wasteful. The other flavor is similar to the aggregation in that the client doesn't just need a single touchpoint, it needs special business logic as well that only applies to this client. Edge services really become very much client-specific gateways, and I prefer them as an overall pattern. They provide the benefits of aggregation, consolidation, and complexity isolation while doing so based on the sole needs of a single client. These edge services focus on a specific client, so their isolation pattern is directly addressing that client. Say we have data that a mobile client doesn't need because it isn't available in the application. When we use an edge service, we can target the payload while allowing the system to scale as needed. Also, not exposing data that the client itself cannot use. Edge design very much follows the gateway pattern as a whole. First, you need to identify the client and the needs of that client, as well as the constraints. Since you have such laser focus on a specific client, you can spend a lot more dedicated energy on that client. Next, based on that information, you build your contracts and associated models. You can then implement the APIs and contracts. One benefit here, especially with the very specific client focus, is that your passivity concerns only last as long as your client version is in scope. This is very powerful in worlds like mobile, where you can drive upgrades from within the application and keep your client usage at a relatively current level. So you can make more non-passive changes by forcing a client upgrade. So at this point, you may wonder if the edge and gateway patterns are so similar, why have both? And this is a great question, and I'll give you a little early hint here. I don't use gateways, I only use edge services. On paper, they are very similar. But by focusing on a single client for the edge, you can apply all of the gateway patterns, but to that client only. But in both cases, we're handling things like isolation, consolidation, aggregating, and proxying. The key is does it apply to the whole system or just one client? The edge becomes much more flexible, in my opinion, because it targets the client specifically. Any aggregation needs can be handled more robustly in an edge service because the client's needs are being taken into account. Consolidation and isolation needs are also more flexible with an edge service for the same reason. A big win for the edge pattern is that you can have a more scalable component. Because you are targeting a single client, as that client load increases or decreases, you can scale your service appropriately. You can also better introduce throttling specific to a client type without applying it to your whole system, which is what you would have to do with the gateway. The edge pattern is also more flexible when new clients come on board. By using the edge pattern, no other systems will be impacted with this advent of a new client. With the gateway, you have to deploy the entire gateway to address the needs of these new clients. With the edge service, you just deploy the new service and the new client picks it up. On the same lines, you won't impact as many other types of clients when you do need to deploy new versions of your edge service because they're so targeted. Now, when it comes to maintenance, the gateway wins. You get consolidation of modules and underlying processes, so keeping clients in sync becomes much easier. You're also deploying fewer instance types across the system, again, improving maintainability. As I stated before, I only use the edge pattern anymore. But I do always use it, and that's the key. Isolation of clients is critical to prevent exposing your entire system to the world and for preventing clients from calling unapproved APIs. Add in OAuth, and you have a very good security footprint on an edge service. In addition, by targeting this way, I can handle any of the needs of a client much more quickly because I know that there's only one component impacted by changing the edge service.


Data Patterns
Single service database

- We will now delve into the world of data patterns in a microservices-based architecture. There are really only a few ways to solve your data needs, but we will start out with what should be the most common pattern, the single service single database pattern. As I stated before, the single-service single-database pattern should be the most common pattern for all data-domain-based services. The concept is actually very simple, but can be cost prohibitive if used with proprietary databases or improperly sized databases. The problem that we are solving with this pattern is that of scalability. In a microservices architecture, the service scalability needs should be proportional to the database's scaling needs. As load on a single service increases, the associated use of the database will also increase. This can cause you to size the database based on single-service peak needs when the entire system is underutilized. In this pattern, each data domain that you build gets its own dedicated data store. The caveat here is that if the domain also is included in an atomic transaction, you have to be a little bit less fine grained. So hard and fast rules seldom apply. It's really best case effort. As your service scales in this pattern, your data store itself also scales, which is another key benefit of this pattern. Since the data domain is isolated and the data store itself is also isolated, you can scale both of them up or down without impacting this system as a whole. If you really build this right, you could actually isolate your data per region while your system still functions as a whole unit. But that's much more advanced than we're going to discuss in this course. So let's visualize what this will look like in a simplified system's architecture. You start with a single service and its single datastore. Now, to color this just a little bit, we'll add two more services, each of which has its own database. Now we introduce load on the first data service, so we scale the services independently of anything else, adding two more instances. This causes increased load on the database, so we need to scale it as well. Now, most often this is accomplished by making the database itself bigger, giving it more IOPS, and in doing so, we've scaled our service independently of any other service, including the data stores. A single-service, single-database model isn't perfect, but the distribution of these types of systems as well as the ability to scale them is worth the effort.

Shared service database

- While the single service, single database pattern is usually a more effective one, there are some times when you must use a shared database. The shared database pattern isn't really new, especially if you've done some application development in your life. In reality, this is an enterprise construct that is carried over into a distributed model like microservices, mainly due to contracts that have already been signed, but not always, as we'll see often in startups. There are some ways that we can pattern our data in our data store to be more effective for eventually breaking the database up. The given in this case is that all data domains exist within a single database instance. We can still treat them as separate databases, however, from both a logic as well as a code concept, even if they're in a single engine. Data distribution should be handled by the database itself. If you're deploying to multiple data centers, you need the database to handle the data synchronization across the data centers themselves. Pushing that to code will cause databases to become out of sync. Even with some more modern databases, replication across WAN gateways is an afterthought, and you may lose database functionality when you consume them. So choose your databases well. While we won't get into the scalability pain points of a single instance database, we can structure our data so that we can isolate it and prepare it for a hopeful breakout one day. To do this, we need to leverage distinct databases in a single engine, or my favorite way, which is to utilize schemas, key spaces, or similar logical groupings within the database engine itself. These strategies allow our code to be written with a single database isolation model, even if the database is shared. To ensure we have proper segmentation of our schema, each service that consumes that schema should have unique credentials. Those credentials should never span the logical breaks. If you have a user that can connect to multiple schemas, you might as well stick with the existing model you have. The idea here is to break it up, even if only logically. - In a similar vein, the data domain should connect to a single schema. Again, we're treating these systems as though they're isolated, even though they aren't. I alluded to this, but this pattern is extremely useful for startups as well. While in startup mode, your focus should be on product market fit. You can spend time building multiple database engines and maintaining them, or you can simply plan for the future while also reducing the overhead with a single engine. Your focus can be on new features, but keeping good isolation in play at the same time. By enforcing this pattern, you can better break the database up at a later point in time because the data and the apps are already logically isolated, even if unfortunately your database has to be shared.


Command Query Responsibility Segregation

- Of all the data patterns for microservices we will discuss in this course, the command query responsibility segregation pattern is by far the most complex of them all. CQRS is one of the most ethereal concepts of data management. Those that get it well and can implement it successfully can dramatically improve their data behavior across the system as a whole. The core pattern we are trying to solve is that our data access patterns diverge from traditional CRUD to a more complex, multi-model pattern within a single bounded context or data domain. This becomes more clear as we have multiple interfaces. In a CQRS model, query interfaces may transform and aggregate the schema to represent the model in a very specific use. However, the write interfaces may inject behavior and infer characteristics based on an entirely different model and set of rules, but the underlying data becomes the same through eventual consistency. Now, obviously this is a very clear divergence from CRUD where a model has a unified set of rules about reading and writing. In CQRS, we diverge dramatically and apply different rules, as I previously stated. As you might imagine, this dramatically increases complexity of the system. When you need this special type of processing, you want to go beyond business processes to accomplish this access pattern. CQRS can be great if implemented correctly, but if it isn't, which unfortunately is common, it can cause a nightmare from a maintainability as well as an operational perspective. So you may ask, when is the complexity worth it in a CQRS model? A common place that it is attempted is with task-based user interfaces. As the write model focuses on the tasks, the read models are based on the system state after the interactions from that task. Because of how these systems work, eventual consistency isn't just something you can live with, it's a must have. You must fully accept that you cannot read data that was just written and be guaranteed an updated state. As such, your use cases must match this model. Furthermore, event-driven models also play well in this use case. When triggers and system events occur from the write operations, the CQRS model works well because reading what was just written doesn't make sense. Now, this is the furthest I'm going to go on this topic with respect for this course. We've exposed it and introduced it, but the reality is that I could spend hours working through the ins and outs of how to build this correctly and it still wouldn't solve the needs of your use case. If you find yourself traveling down this road, do some homework on the topic and spend some significant time in design before you even create the empty project that will house your code. This is a very complex topic, and if you read many blog posts, you'll see failure after failure of implementing this because it's not easy. But like I've said before, when it's done right, it is extremely powerful, and it reduces the complexity of your access patterns in many cases, even though this component itself becomes much more complex.


Asynchronous eventing

- The final data access pattern we will discuss is asynchronous eventing. While it needs to be talked about as a pattern, it is much bigger than this course. Check out my course in the library, Microservices: Asynchronous Messaging, to dive much deeper into this topic. Many times in a microservices architecture, you run into situations with long-running transactions or complex workflows that just cannot fit into a single blocking API call. Enter into the picture asynchronous eventing. The problems we can solve go beyond what I just described, but essentially in a nutshell, we can describe the superset of problems as some process that cannot be done in real time through a blocking call. The pattern deployed depends upon the use cases, but they all share a common thread, a service API that triggers the event. The event can cascade asynchronously from the API itself, putting together a series of actions that all happen behind the scenes after the client has received an accepted message from the service API of course. Oftentimes, you can make a single blocking call and put a message on a messaging system. Once that is done, the service returns and then the behind-the-scenes processing occurs in an asynchronous manner. One key characteristic of this model is how powerful it can be in a distributed system. Nothing is a silver bullet when it comes to software, but there are many complex problems in distributed systems that asynchronous eventing can solve. As I stated before, I encourage you to take the Microservices: Asynchronous Messaging course for a more detailed view of the why we do this, as well as the how to do this for this complex set of patterns. I just want to leave you with this. Even though I'm deferring this topic to another course, its power cannot be understated. I have used this class of operations more times than I can count when building out systems. Things like business processing, auditing, security, eventing, infrastructure creations, as well as configurations, just touch the surface. Other than how you manage your data domains, this may be the most important pattern you can learn.


Opearation Patterns
Log aggregation patterns

- Now we're going to dive into our final set of patterns, which are operational patterns. Now, these are unique in that they're more about how to run your system than how to build your system, but these are tried and true methods. The first one that we're going to talk about is log aggregation. So I'll dive into this pattern by first talking about the source of that data, and that is the log messages themselves. The problem that we are trying to solve is that operationally, we need to know what is going on with our systems. When errors occur, we need a way to quickly diagnose and resolve the errors, and logs are one of the single best places to look. Logs are invaluable if done right. Logs provide detailed information of the runtime behavior of your microservice artifact. Through other operational patterns that we'll talk about, Logs can be written and linked to other system logs as well. As such, logging must be consistent across all services. As we start doing log aggregation and indexing consistency, not only within a single service, but across the entire system becomes critical. The structure and format of your logs therefore must be consistent and enforced across the whole system. While polyglot programming can make this more difficult, the time spent making your log output consistent will be well worth it. Beyond simple structure and possibly more important because the ability to process logs via aggregation is the need for a common taxonomy. Domain level taxonomy and logs should be shared across every consumer and related service. Identifiers and core values across your log messages should be keyed the same, as well as documented so that everyone knows what those keys are, what keys they should be using, and when to include this data in the log messages. Log aggregation is a key pattern in the operations of a microservices based system. In monolithic models, the log messages usually all go to the same output. As such, they essentially are aggregated on their own. With microservices, however, it's a different thing. The problem we need to solve is that our logs are everywhere. Each service writes its own logs and dumps them to the output of some kind. The output locations usually are unique per service and can be very difficult to follow in a stateless flow. Log aggregation is, as you might imagine, a way to aggregate the various output locations into a single stream of data. This aggregation takes on many forms, but the key here is that at the end, the logs all end up in one place. Part of that aggregation can and should be parsing of the logs. This is where the log structure is so important. Logs usually have a message and a bunch of metadata called tags that are associated with that message. The more structured the logs are, the less parsing you have to do on them because of a well-defined set of rules. The reality is that the faster you can get your logs aggregated, the faster they can be indexed. Then you can actually use that log data to diagnose issues. Correlation of log messages is where the real power comes into play and why things like structure and taxonomy are so important. We will talk about tracing later, but ensuring tracing identifiers are injected into logs the exact same way across every service with the same key will allow you to recreate call stacks of errant processes. In addition, taxonomy will help you identify issues as well as you have a clear picture of what these identifiers actually mean. Logging as a whole is one of those things that you should just use off the shelf components for. Structured logging frameworks exist and almost every language has them. Log parsing and shipping exists for almost every common aggregation platform. You just need to build a common taxonomy and document it, and then let the tools do their job across your system as a whole.


Metrics aggregation patterns

- In a similar vein to log aggregation, operationally, metrics are an invaluable tool to diagnosing issues. In fact, metrics can be more operationally useful than logging if used correctly. Metrics are also easier than logging because there's really less human interaction. Logging relies on a developer to write and structure the log message. Metrics, however, usually just requires a bit of instrumentation. The problem we're looking to solve is that we need to know what is going on with the system as a whole, while being able to peek into individual services. We aren't really looking at code output. We're looking for system output. Once again, a common taxonomy is critical with metrics. Structuring your keys to be consistent and descriptive will help explain what the ultimate dashboards are saying. Having common keys with metrics and logs is even a better strategy. As with logging, much has been done already and there are standard metrics libraries available in almost every language. While much of the metrics you collect are system level and runtime component data points, you may want to also decorate your metrics dashboards with custom data. Metrics shipping is accomplished, again, via standard componentry. Most metrics aggregation solutions have one or more shipping standards that already exist, so just use them. Once your metrics are shipped, you can view them usually graphically through a metrics aggregation solution. Dashboards are powerful, especially for on-call activity. Metrics aggregation is relatively simple, but it is critical in an operational model. I have learned the hard way to not ignore metrics ever. As such, I want to share with you a few lessons I have learned over time and how I use them still to this day. First and foremost, I would recommend you build very high level dashboards. Your on-call engineer can use these high level dashboards as a place to see trouble starting, before it takes hold. Don't make these dashboards too busy, but make them busy enough that you can see the true health of your system. In addition to those high level dashboards, you want to build detailed dashboards for each service. The high level is an aggregated and simplified view, but once you think you have a problem, the detailed dashboard can provide much more insight into the service. I usually embed a link to log queries in my metrics dashboards, because at two in the morning, building a log query with the stress of a potential outage, after being woken up from sleep, isn't very appealing. Consider also injecting events into your dashboards. System events that can impact system performance, as well as user events, can help you understand what systems are being impacted. In addition, deployment events can help you understand that you have code issues that are impacting the system as a whole based on a newly introduced code. If you can, put traces on your graphs at alarm points. If you're going to get paged, having a reason for the page visually can help promote a quicker resolution. Again, consider the middle of the night scenario of being woken up by a page. Finally, ensure you have runbooks for every alarm and explanations of what metrics and logs to look at. Metrics and log aggregations, both are about identifying as well as resolving issues. If you build an alarm, you know why the alarm fired. The runbook will make troubleshooting significantly easier. I embed links to runbooks into the pages themselves, as well as the dashboards. Again, making life easier for on-call, which ultimately is the point of all operational patterns. Solid, well-defined metrics will help you understand your system, its errors, its health, and ultimately how you are treating your users.


Tracing patterns

- In a microservices-based system, call stacks can quickly get lost. Enter into the picture, tracing patterns. Tracing is one of those things that once you build it, you'll regret not having it before that time. Consider for a moment a monolithic system. All code execution, from edge to database call, was in a single process. As such, a stack trace will help you recreate the path that the service call took. In a microservices architecture, however, that is gone because calls span processes as well as the network, not just functions. Tracing gives you a way to recreate the call stack by injecting a trace identifier into every call. The tracing identifier should be injected at the edge and span all the way to the database call, or even to the database itself, if possible. By spanning the stack, you can inject the trace ID into logs, and that will help you further diagnose issues. By leveraging tracing, no call is ever lost as long as everyone honors the trace identifier. Implementations of tracing are relatively straightforward. First and foremost, use a standards-based approach. I have worked in environments where a pattern was homegrown, and to be honest, it was painful. Open standards allow you to use off-the-shelf tooling to introspect your system. The trace ID should be injected into the entry point of your system, be it a browser or an edge service or any other mechanism, even something like an ETL. Every log message should embed the trace ID. Open standards use headers, so your logging functions should simply pull it out of the header and drop it into the tag on the log message using common taxonomy. Leveraging tracing tools and APM, or application performance monitoring, can help you visualize the call stacks, which can be helpful to many people when that two a.m. page hits your phone. As with logging in metrics, don't recreate the wheel. There are very good patterns out there for systems that all play nicely with microservices. Just use what already exists, but use it.


External configuration

- Externalized configuration in a microservices architecture isn't a hard and fast requirement like it is for cloud native, but its value becomes operationally important when load and movement of services occurs. The value proposition in externalized configuration in microservices is more about the operations than distribution. When you have systems running and issues occur, having a clear spot to see configuration outside of code can greatly impact the meantime to resolution. Externalization of your configuration varies based on your runtime as well as the frameworks being used. Runtimes like Kubernetes and frameworks like Spring have sophisticated models to inject configuration into your application, but many other tools do as well. The key here is to use tooling that makes external environment variables easy to find, as well as manipulate. While it may not seem like a huge deal, consistent naming is also very helpful. Again, always look at operational patterns from the perspective of the on-call engineer being woken up in the middle of the night. If you consistently call a variable X, except in one case when you get a page on that one case and you will, you may find that your resolution time suffers. When defining which values for configuration should be externalized, my recommendation is to always err on the side of externalization. It doesn't hurt to expose too much config, but if you don't expose enough, resolution or troubleshooting may suffer. On that line, however, there's a need to protect secrets. Secrets can and should be externalized in most implementations through special constructs. But ensure the on-call team has a path to get them securely while not exposing them to prying eyes. Externalized config is not a hard concept, but I do want to talk about the pattern and how applies. First, config is usually injected into your service or retrieved as part of your startup routine for your framework. These values then become available to your code through various mechanisms, and each language has very specific ways of accessing these values. Your service should then be written to utilize these values in favor of embedded values. I say embedded because I do believe in the use of defaults where they make sense. Once again, there's a lot of common libraries, toolings and patterns within each language that will help you consume, use, and flex behavior based on various values. Once again, use what already exists. The basic pattern, however, is always the same. Read the config, flex your behavior as necessary and act. It's simple enough.


Service discovery

- As a microservices architecture grows, so does the complexity of finding the appropriate services that provide the models and behaviors that you need. Enter into the picture Service Discovery. Service discovery can save you a lot of time, especially in a dynamic runtime where service locations can vary based on scaling events or geographic location. The problem we are trying to solve here is very simply, what service do I need to call in order to perform some body of work? When you only have a small set of microservices, that may seem very manageable within your configuration. However, once you go to hundreds or even thousands within a single environment? It's not so manageable. The concept is actually quite simple as well. A central location exists that allows for discovery of services. As a new service comes online, it advertises itself and what models or behaviors it exposes to the rest of the system. It does this by in introspection of its URI, essentially saying, "At location foo, you can find services for bar." A service can then query the central location to find out which other services expose the needed behavior and what their location is. Once it's found, a URI to that service is returned from the central location. And it's all based on the aforementioned advertisement. The service has then been discovered and the client can call the URI from discovery instead of configuration. This model becomes much more extensible than simple config. Building a discovery system in clients, however, is beyond the scope of this course. But I will say, off the shelf components exist. Use them if possible. Netflix, for instance, has done a great job of open sourcing many of their components around service discovery. And they're an example of a company running thousands and thousands of microservices, all based on that service discovery. You can start off without this pattern and even run some systems of some size without it. However, you may get to a point where the value of this pattern really shines. The flexibility alone gives it some of that value. Keep this pattern in your tool belt for when you need it.


Continuous delivery

- When talking about continuous delivery, we need to start with the definition. Continuous delivery is a process by which we constantly deliver new code to production with, in my opinion, full automation. This is really where this operational pattern, that does exist outside of microservices, really shines. The concept is basically to take the artifacts that were previously built and move them through your non-production environments all the way to production using automated gates. The devil is in the details, however. I want to share with you some of the strategies I have successfully used in my career to achieve this. Once an artifact has been built and has its unit tests run, the artifact is published. This is the trigger that I use to move this code into non-production environments, that process should be seamless and fully automated. Once you have your code in non-prod, the work actually begins. One of the first strategies I use is automated integration tests. Now, I'm a big fan of leveraging an API layer to expose a small subset of my services to the outside world. Because of this, many of my service endpoints are hidden to the outside, including testing tools. The strategy I use is deploying integration test suites into the environment along with the code itself. These integration test suites exist as single run jobs in my environment. They set up the data conditions and then execute against my APIs exposed in the code deployment. They do a lot of logging. Since these processes are hard enough to debug, once they pass, the automated gate is cleared. I then add in security testing, it comes in two forms. I use third party tools for both of them. The first flavor is penetration testing. I execute my penetration testing framework on every single deployed and non-prod, not just the UI components. This gives me confidence that any change introduced doesn't impact the overall security of the environment. I then run internal security tests that execute the exact same as my integration tests. These tests are really about stress testing the most likely areas of security vulnerabilities. CI/CD OWASP top 10 for some ideas here. The final set of tests that I automated in non-production is user acceptance testing. These not only use an external framework, but need a lot of care as the UIs change often. We find the benefit here is proportional to the work that we put into them. So they again, help us have confidence in moving code to production. Now, we execute each of these suites of tests in every environment up to production. Once we get into production, we drop our UAT, but we have cron operation that continually smoke tests our applications with the most popular workloads, keeping our confidence level high. Now, I don't want to pretend like this is a new concept, however, it's one of the most important aspects for microservices. In my opinion, CI/CD is a requirement of microservices, more so than any other way of developing applications. So building out strong automation patterns is worth the time and energy.


Documentation

- Documentation is always the last or even forgotten concept in almost all of software. However, with the microservices architecture, it really is critical. Your system may be written by multiple teams with very different focus areas, and having a solid documentation for your APIs can aid in the inter-team communication that will need to happen. In fact, great documentation can turn a stressful situation into a more collaborative event. Now, documenting APIs is nothing new, you should use any pattern that is native to your service definition. For instance, if you are writing Restful services, OpenAPI is a great tool for documenting your contracts. Now, I'm an old school, contract-first type developer, but that is in no way the only way to do it. Most languages and frameworks have integrations for OpenAPI that let you do both contract-first development as well as code-first development. The real niche area of this pattern, however, is how you share that documentation. I believe in a two-pronged approach to solving this concern. First, I believe each service should expose its documentation at a consistent location. This means that an HTTP get request at a specific URL, off of the context route, say /docs, will yield an HTML renderable documentation page. Check the documentation framework for the tools you are using. Most likely, this is an easy switch to just turn on and get this. The key here is that every API renders its documentation at the same URL. Now, from there, you can populate your central documentation repository. If you automate this step, that's ideal, but you at least need to publish the documentation. Automation is pretty simple too here. You build a system that leverages tools like Service Discovery. You can then get a collection of every service, and because they all expose docs at the same URL, you can crawl them and publish the documentation updates. Consider examples of API consumption in your documentation as well. This really helps the caller know how to use your API. Consider also publishing your team name and direct contacts within the documentation. This way, a consumer knows who to talk to if they have questions. The power of solid documentation directly impacts operations, agility, and improves reuse of code. More importantly, in my mind, it helps ensure the code and its APIs are well discussed as these documentation changes should be part of your pull requests.